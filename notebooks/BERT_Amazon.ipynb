{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "cellId": "doniog759xdytabcsdf8fj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "cellId": "qrckypn014xiywkv4al6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oYsV4H8fCpZ-",
    "outputId": "2e60467d-6a6b-4898-e2f4-f42884cc6092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "x6qbstw9pgas2lqs5np1fh"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "wxcm6vmcjqsnipymls1jl9"
   },
   "source": [
    "we will use Amazon product reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "cellId": "uviill0hfmigz8o1b9gu"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "#lets load data\n",
    "# to reproduce, download the data and customize this path\n",
    "PATH_TO_DATA = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "cellId": "cjtovd92aum7wl7t0iihh2"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "cellId": "fksspqp907t496byc7kpl9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(PATH_TO_DATA + 'train.csv', index_col='id').fillna('')  # обучающий датас\n",
    "valid_df = pd.read_csv(PATH_TO_DATA + 'valid.csv', index_col='id').fillna('')  # валидационный с таргетом\n",
    "test_df = pd.read_csv(PATH_TO_DATA + 'test.csv', index_col='id').fillna('')  # тестовый для каггла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "cellId": "xxbpdsycfgphxhamz1a3t9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52057, 2), (17353, 2), (17353, 1))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "cellId": "sla5mmhslxov0mcfqiaknd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sam has an everlast treat each nite before bed...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The product is as it says. I keep an eye on it...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My Kitty thinks these are treats! He loves the...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is the third or fourth time that we've or...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Put this on both my dogs. And they are scratch...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text label\n",
       "id                                                         \n",
       "0   Sam has an everlast treat each nite before bed...  dogs\n",
       "1   The product is as it says. I keep an eye on it...  dogs\n",
       "2   My Kitty thinks these are treats! He loves the...  dogs\n",
       "3   This is the third or fourth time that we've or...  dogs\n",
       "4   Put this on both my dogs. And they are scratch...  dogs"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "cellId": "c3qvm624jran72pmuj05xs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dogs                    0.537872\n",
       "cats                    0.355284\n",
       "fish aquatic pets       0.069001\n",
       "birds                   0.020324\n",
       "bunny rabbit central    0.010950\n",
       "small animals           0.006570\n",
       "Name: label, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_df.label.value_counts(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "nq0q6vcjsrdoxwni4jfzpe"
   },
   "source": [
    "мы видим, что больше половины наблюдений - про собак, треть про кошек, остальные относятся к рыбкам, птицам, кроликам и другим небольшим животным"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "zl8g9u7g1jx661pdksvpq"
   },
   "source": [
    "#!g1.1\n",
    "категории имеют существенный дисбаланс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ojyoxtdlnfknganup05c",
    "id": "4SMZ5T5Imhlx"
   },
   "source": [
    "\n",
    "\n",
    "Let's extract the sentences and labels of our training set as numpy ndarrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "cellId": "m34oe1hw05p1dhexpqhi1c",
    "id": "GuE5BqICAne2"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Get the lists of sentences and their labels.\n",
    "sentences = train_df.text.values\n",
    "labels = train_df.label.values\n",
    "\n",
    "#valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "cellId": "7i0tablp5rjkpyfr7u1i5m"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# закодируем таргет\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "labels = le.fit_transform(train_df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "cellId": "b7myno8zx9hw89g97e9iui"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "cellId": "ypvg503k829hei6kpf57wt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 2, 3, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "n2m4dcnkfdaicq7fwl9c6o",
    "id": "ex5O1eV-Pfct"
   },
   "source": [
    "# 3. Tokenization & Input Formatting\n",
    "\n",
    "In this section, we'll transform our dataset into the format that BERT can be trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "qtzfq4mx3uw401xg02zbn",
    "id": "-8kEDRvShcU5"
   },
   "source": [
    "## 3.1. BERT Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "nltqt9p18y4hwrqoid46m",
    "id": "bWOPOyWghJp2"
   },
   "source": [
    "\n",
    "To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n",
    "\n",
    "The tokenization must be performed by the tokenizer included with BERT--the below cell will download this for us. We'll be using the \"uncased\" version here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "cellId": "6jmucnexl0vfmaq78346gl",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "82ddfcea0e4c4e5a86cf6eca8585be8d",
      "8a256ba4a19e4ec98fe3c3c99fba4daa",
      "8c76faadf2f4415393c6f0a805f0d72b",
      "e0bb735fda99434a90380e7fc664212d",
      "cdb78e75309f4bc09366533331e72431",
      "1058e0b5baa248faa60c1ad146d10bf7",
      "375cc635389c4ddb9bf2aa443df58bae",
      "472198d5b6a748b3a81f9364fd1fa711"
     ]
    },
    "id": "Z474sSC6oe7A",
    "outputId": "4e6d97b6-2d4c-42ca-c201-d2b4a88895b9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ee8a6d0029414cbff2ad600c54f9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b8aea4816840449feac268e0b627e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30ae30eb8164e7594ac852d665a3393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d3211df7324101bbc04000ba89b96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=483.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# попробуем облегченную модель \n",
    "# Load the BERT tokenizer. distilbert-base-uncased\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)  #стандартная модель - bert-base-uncased, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "f6ic2xeksek12tly7v2t7yh",
    "id": "dFzmtleW6KmJ"
   },
   "source": [
    "Let's apply the tokenizer to one sentence just to see the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "axk01lz69tq81o7ofqc93h",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLIbudgfh6F0",
    "outputId": "9ca681ff-195f-4960-a0ba-55ded440278e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[10])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[10]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "f0bzhx87zoslojylqdjd",
    "id": "l6w8elb-58GJ"
   },
   "source": [
    "## 3.3. Tokenize Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "pa70o6ujlergqrti32dqxq",
    "id": "U28qy4P-NwQ9"
   },
   "source": [
    "Оценим максимальную длину текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "cellId": "xlo28mhoywpnguhvgxvoi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKsH2sU0OCQA",
    "outputId": "e363e816-c750-422f-b623-dce428f77502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  2971\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "cellId": "sc4wjjpopbj8igqtgdtv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    52057.000000\n",
       "mean        84.420443\n",
       "std         80.027988\n",
       "min          1.000000\n",
       "25%         35.000000\n",
       "50%         61.000000\n",
       "75%        106.000000\n",
       "max       2360.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "# тем не менее, важно понимать распределение кол-ва токенов в последовательности:\n",
    "\n",
    "# statistics of text length (in words)\n",
    "train_df['text'].apply(lambda s: len(s.split())).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "cellId": "t6bczuhsuk8aiqh6zlaj6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229.0, 392.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "train_df['text'].apply(lambda s: len(s.split())).quantile(0.95), train_df['text'].apply(lambda s: len(s.split())).quantile(0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "8shl97fmpl98n44gdsyjdh"
   },
   "source": [
    "#!g1.1\n",
    "99  персентиль - 392 токена, мы можем обрезать использовать стандартные 512, или взять 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "i755qyr606mma331z1j2bj",
    "id": "tIWAoWL2RK1p"
   },
   "source": [
    "Now we're ready to perform the real tokenization.\n",
    "\n",
    "The `tokenizer.encode_plus` function combines multiple steps for us:\n",
    "\n",
    "1. Split the sentence into tokens.\n",
    "2. Add the special `[CLS]` and `[SEP]` tokens.\n",
    "3. Map the tokens to their IDs.\n",
    "4. Pad or truncate all sentences to the same length.\n",
    "5. Create the attention masks which explicitly differentiate real tokens from `[PAD]` tokens.\n",
    "\n",
    "The first four features are in `tokenizer.encode`, but I'm using `tokenizer.encode_plus` to get the fifth item (attention masks). Documentation is [here](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "cellId": "yytmmzbcb1zzlhqavefsg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bBdb3pt8LuQ",
    "outputId": "b4d78c6d-0faf-459b-b11a-a26ce40bd32a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Sam has an everlast treat each nite before bed, like a good tooth brushing. The only downside is finding a place that keeps them in stock as well as multiple flavors.\n",
      "Token IDs: tensor([  101,  3520,  2038,  2019,  2412,  8523,  2102,  7438,  2169,  9152,\n",
      "         2618,  2077,  2793,  1010,  2066,  1037,  2204, 11868, 12766,  1012,\n",
      "         1996,  2069, 12482,  5178,  2003,  4531,  1037,  2173,  2008,  7906,\n",
      "         2068,  1999,  4518,  2004,  2092,  2004,  3674, 26389,  1012,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Pad & truncate all sentences.   Установили на основе анализа длины текстов\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "82676h6h03u8rs5mvnowla",
    "id": "aRp4O7D295d_"
   },
   "source": [
    "## 3.4. Training & Validation Split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "iuprzz80qrdsfz1b97rtk",
    "id": "qu0ao7p8rb06"
   },
   "source": [
    "Divide up our training set to use 90% for training and 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "cellId": "ib4pjswxwwzpdbvvlplze",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GEgLpFVlo1Z-",
    "outputId": "c0ae3d66-6982-4c33-a3f4-ca80e0cd9968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46,851 training samples\n",
      "5,206 validation samples\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "m9ybm4alx44tp9k0we34n",
    "id": "dD9i6Z2pG-sN"
   },
   "source": [
    "We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "cellId": "iwqd3ad1js80vl8i354nnl",
    "id": "XGUqOCtgqGhP"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 64  # посмотрим, как зайдет по памяти 64\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "66cyrih7c3p0cl88mzgvetq",
    "id": "8bwa6Rts-02-"
   },
   "source": [
    "# 4. Train Our Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "imoti25fzyfya532kf959",
    "id": "3xYQ3iLO08SX"
   },
   "source": [
    "Now that our input data is properly formatted, it's time to fine tune the BERT model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "s06oty1c87j31dvpuqspgj",
    "id": "D6TKgyUzPIQc"
   },
   "source": [
    "## 4.1. BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "s4gtg8ptifgsq4bdfylkhe",
    "id": "WnQW9E-bBCRt"
   },
   "source": [
    "OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\").\n",
    "\n",
    "The documentation for `from_pretrained` can be found [here](https://huggingface.co/transformers/v2.2.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained), with the additional parameters defined [here](https://huggingface.co/transformers/v2.2.0/main_classes/configuration.html#transformers.PretrainedConfig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "cellId": "ad2llmews7fmoi1mypcp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_df.label.nunique()  # 6 уникальных категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "cellId": "eulbqmcr2ujok8i7zlmsq",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "bf9dfa1ff3e642fbb74c5146d21044c2",
      "1c2b0ede959142fc89bf07a9c88df638",
      "1296a3d754b344a482a03e5af84e805e",
      "6f132d7bb83d41b6847df0d0ec0a1b92",
      "2755b9838bae408ca8cf667ad9d501fc",
      "f8874fec8a404ae89a38fd2ecbb357cf",
      "a7bdbedc75de4f77b45f1389c2ea0abc",
      "978c24b18b594eaf8ca47730a88eefb9",
      "fe254c3bcc08402eb506f0e98f5673a7",
      "cea84f9c3db641acb98314028b305514",
      "23ca9359e6c44232a1346e6f2ab7e48c",
      "d689bc8d488a4dc09c393b4fc9747bcb",
      "6c7dec7b1e804c2195f6e60fb3c1d18e",
      "0fe5b1d0540240a8a8426352c24b2887",
      "4b1e27aff6f04fec8268d951e46b1e63",
      "440da34c72344cb08e4a1ee5de7049ee"
     ]
    },
    "id": "gFsCTp_mporB",
    "outputId": "af690f33-6cd5-4678-bdaf-209f068f70f5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee371fe7289941f0ae0837d763996dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=483.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be95f97de08d49d79a6869a7e12b6360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'vocab_projector.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'vocab_layer_norm.bias', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.10.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'classifier.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'classifier.weight', 'encoder.layer.3.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.6.attention.self.value.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.query.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.7.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "#НУЖНО УКАЗАТЬ КОЛ-ВО КЛАССОВ\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab. используем дистилированный берт\n",
    "    num_labels = 6, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "zwt8dfat1yn93b1dbyv65",
    "id": "e0Jv6c7-HHDW"
   },
   "source": [
    "Just for curiosity's sake, we can browse all of the model's parameters by name here.\n",
    "\n",
    "In the below cell, I've printed out the names and dimensions of the weights for:\n",
    "\n",
    "1. The embedding layer.\n",
    "2. The first of the twelve transformers.\n",
    "3. The output layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "cellId": "i5mvxfy29xh3w280j4ppt",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PIiVlDYCtSq",
    "outputId": "7430f38d-de86-4488-bb92-6a9b0142b3af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (6, 768)\n",
      "classifier.bias                                                 (6,)\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "tlfcqv02mm9z73ta5kclm",
    "id": "qRWT-D4U_Pvx"
   },
   "source": [
    "## 4.2. Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "vkncfex3bwirec6q7ay3a",
    "id": "8o-VEBobKwHk"
   },
   "source": [
    "Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n",
    "\n",
    "For the purposes of fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf)):\n",
    "\n",
    ">- **Batch size:** 16, 32  \n",
    "- **Learning rate (Adam):** 5e-5, 3e-5, 2e-5  \n",
    "- **Number of epochs:** 2, 3, 4 \n",
    "\n",
    "We chose:\n",
    "* Batch size: 32 (set when creating our DataLoaders)\n",
    "* Learning rate: 2e-5\n",
    "* Epochs: 4 (we'll see that this is probably too many...)\n",
    "\n",
    "The epsilon parameter `eps = 1e-8` is \"a very small number to prevent any division by zero in the implementation\" (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
    "\n",
    "You can find the creation of the AdamW optimizer in `run_glue.py` [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "cellId": "3q01ima1jp3292ro3a39aq",
    "id": "GLs72DuMODJO"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "cellId": "wcx8dw4f53a7e98jomhwb2",
    "id": "-p0upAhhRiIx"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "4eub57f4047mh1jxx1jn9l",
    "id": "RqfmWwUR_Sox"
   },
   "source": [
    "## 4.3. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "edt6a10duzijfihsf4ahhr",
    "id": "_QXZhFb4LnV5"
   },
   "source": [
    "Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase. \n",
    "\n",
    "> *Thank you to [Stas Bekman](https://ca.linkedin.com/in/stasbekman) for contributing the insights and code for using validation loss to detect over-fitting!*\n",
    "\n",
    "**Training:**\n",
    "- Unpack our data inputs and labels\n",
    "- Load data onto the GPU for acceleration\n",
    "- Clear out the gradients calculated in the previous pass. \n",
    "    - In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.\n",
    "- Forward pass (feed input data through the network)\n",
    "- Backward pass (backpropagation)\n",
    "- Tell the network to update parameters with optimizer.step()\n",
    "- Track variables for monitoring progress\n",
    "\n",
    "**Evalution:**\n",
    "- Unpack our data inputs and labels\n",
    "- Load data onto the GPU for acceleration\n",
    "- Forward pass (feed input data through the network)\n",
    "- Compute loss on our validation data and track variables for monitoring progress\n",
    "\n",
    "Pytorch hides all of the detailed calculations from us, but we've commented the code to point out which of the above steps are happening on each line. \n",
    "\n",
    "> *PyTorch also has some [beginner tutorials](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) which you may also find helpful.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "u98y155y7cd8cupgb208g",
    "id": "pE5B99H5H2-W"
   },
   "source": [
    "Define a helper function for calculating accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "cellId": "pm58rxf5eena1gqat4x3rk",
    "id": "9cQNvaZ9bnyy"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "xh2gi91vyzsw4g6h3s7jdi",
    "id": "KNhRtWPXH9C3"
   },
   "source": [
    "Helper function for formatting elapsed times as `hh:mm:ss`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "cellId": "whbwtueefmf6cyv4kjctb",
    "id": "gpt6tR83keZD"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "3kyi9m0kxhx5tfm5oswuum",
    "id": "cfNIhN19te3N"
   },
   "source": [
    "We're ready to kick off the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "cellId": "8pf7qzfvneap0oeobtn11q",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6J-FYdx6nFE_",
    "outputId": "b2c3e30b-eb5d-4b13-a207-05a48a87ed2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    733.    Elapsed: 0:00:36.\n",
      "  Batch    80  of    733.    Elapsed: 0:01:10.\n",
      "  Batch   120  of    733.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    733.    Elapsed: 0:02:17.\n",
      "  Batch   200  of    733.    Elapsed: 0:02:51.\n",
      "  Batch   240  of    733.    Elapsed: 0:03:25.\n",
      "  Batch   280  of    733.    Elapsed: 0:03:58.\n",
      "  Batch   320  of    733.    Elapsed: 0:04:32.\n",
      "  Batch   360  of    733.    Elapsed: 0:05:06.\n",
      "  Batch   400  of    733.    Elapsed: 0:05:40.\n",
      "  Batch   440  of    733.    Elapsed: 0:06:13.\n",
      "  Batch   480  of    733.    Elapsed: 0:06:47.\n",
      "  Batch   520  of    733.    Elapsed: 0:07:21.\n",
      "  Batch   560  of    733.    Elapsed: 0:07:55.\n",
      "  Batch   600  of    733.    Elapsed: 0:08:28.\n",
      "  Batch   640  of    733.    Elapsed: 0:09:02.\n",
      "  Batch   680  of    733.    Elapsed: 0:09:36.\n",
      "  Batch   720  of    733.    Elapsed: 0:10:10.\n",
      "\n",
      "  Average training loss: 0.77\n",
      "  Training epcoh took: 0:10:20\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.81\n",
      "  Validation Loss: 0.54\n",
      "  Validation took: 0:00:26\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    733.    Elapsed: 0:00:34.\n",
      "  Batch    80  of    733.    Elapsed: 0:01:08.\n",
      "  Batch   120  of    733.    Elapsed: 0:01:41.\n",
      "  Batch   160  of    733.    Elapsed: 0:02:15.\n",
      "  Batch   200  of    733.    Elapsed: 0:02:49.\n",
      "  Batch   240  of    733.    Elapsed: 0:03:23.\n",
      "  Batch   280  of    733.    Elapsed: 0:03:56.\n",
      "  Batch   320  of    733.    Elapsed: 0:04:30.\n",
      "  Batch   360  of    733.    Elapsed: 0:05:04.\n",
      "  Batch   400  of    733.    Elapsed: 0:05:38.\n",
      "  Batch   440  of    733.    Elapsed: 0:06:12.\n",
      "  Batch   480  of    733.    Elapsed: 0:06:45.\n",
      "  Batch   520  of    733.    Elapsed: 0:07:19.\n",
      "  Batch   560  of    733.    Elapsed: 0:07:53.\n",
      "  Batch   600  of    733.    Elapsed: 0:08:27.\n",
      "  Batch   640  of    733.    Elapsed: 0:09:00.\n",
      "  Batch   680  of    733.    Elapsed: 0:09:34.\n",
      "  Batch   720  of    733.    Elapsed: 0:10:08.\n",
      "\n",
      "  Average training loss: 0.50\n",
      "  Training epcoh took: 0:10:18\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.82\n",
      "  Validation Loss: 0.50\n",
      "  Validation took: 0:00:23\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    733.    Elapsed: 0:00:34.\n",
      "  Batch    80  of    733.    Elapsed: 0:01:08.\n",
      "  Batch   120  of    733.    Elapsed: 0:01:41.\n",
      "  Batch   160  of    733.    Elapsed: 0:02:15.\n",
      "  Batch   200  of    733.    Elapsed: 0:02:49.\n",
      "  Batch   240  of    733.    Elapsed: 0:03:23.\n",
      "  Batch   280  of    733.    Elapsed: 0:03:57.\n",
      "  Batch   320  of    733.    Elapsed: 0:04:30.\n",
      "  Batch   360  of    733.    Elapsed: 0:05:04.\n",
      "  Batch   400  of    733.    Elapsed: 0:05:38.\n",
      "  Batch   440  of    733.    Elapsed: 0:06:12.\n",
      "  Batch   480  of    733.    Elapsed: 0:06:45.\n",
      "  Batch   520  of    733.    Elapsed: 0:07:19.\n",
      "  Batch   560  of    733.    Elapsed: 0:07:53.\n",
      "  Batch   600  of    733.    Elapsed: 0:08:27.\n",
      "  Batch   640  of    733.    Elapsed: 0:09:01.\n",
      "  Batch   680  of    733.    Elapsed: 0:09:34.\n",
      "  Batch   720  of    733.    Elapsed: 0:10:08.\n",
      "\n",
      "  Average training loss: 0.41\n",
      "  Training epcoh took: 0:10:18\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n",
      "  Validation Loss: 0.42\n",
      "  Validation took: 0:00:23\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    733.    Elapsed: 0:00:34.\n",
      "  Batch    80  of    733.    Elapsed: 0:01:08.\n",
      "  Batch   120  of    733.    Elapsed: 0:01:41.\n",
      "  Batch   160  of    733.    Elapsed: 0:02:15.\n",
      "  Batch   200  of    733.    Elapsed: 0:02:49.\n",
      "  Batch   240  of    733.    Elapsed: 0:03:23.\n",
      "  Batch   280  of    733.    Elapsed: 0:03:57.\n",
      "  Batch   320  of    733.    Elapsed: 0:04:30.\n",
      "  Batch   360  of    733.    Elapsed: 0:05:04.\n",
      "  Batch   400  of    733.    Elapsed: 0:05:38.\n",
      "  Batch   440  of    733.    Elapsed: 0:06:12.\n",
      "  Batch   480  of    733.    Elapsed: 0:06:46.\n",
      "  Batch   520  of    733.    Elapsed: 0:07:19.\n",
      "  Batch   560  of    733.    Elapsed: 0:07:53.\n",
      "  Batch   600  of    733.    Elapsed: 0:08:27.\n",
      "  Batch   640  of    733.    Elapsed: 0:09:01.\n",
      "  Batch   680  of    733.    Elapsed: 0:09:35.\n",
      "  Batch   720  of    733.    Elapsed: 0:10:08.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epcoh took: 0:10:19\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n",
      "  Validation Loss: 0.41\n",
      "  Validation took: 0:00:23\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:42:53 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
    "        # function and pass down the arguments. The `forward` function is \n",
    "        # documented here: \n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "        # The results are returned in a results object, documented here:\n",
    "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
    "        # Specifically, we'll get the loss (because we provided labels) and the\n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "        # output values prior to applying an activation function like the \n",
    "        # softmax.\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ly9tdk1mg6n602zpsp5is",
    "id": "VQTvJ1vRP7u4"
   },
   "source": [
    "Let's view the summary of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "cellId": "xsqyj72ytdkr64ua4zfzl",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "6O_NbXFGMukX",
    "outputId": "a9e51eda-5eae-4800-87d5-8d016ff25bb2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0:10:20</td>\n",
       "      <td>0:00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0:10:18</td>\n",
       "      <td>0:00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0:10:18</td>\n",
       "      <td>0:00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0:10:19</td>\n",
       "      <td>0:00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.77         0.54           0.81       0:10:20         0:00:26\n",
       "2               0.50         0.50           0.82       0:10:18         0:00:23\n",
       "3               0.41         0.42           0.84       0:10:18         0:00:23\n",
       "4               0.37         0.41           0.84       0:10:19         0:00:23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "cellId": "ymqquyyha6r7crmuw62ghu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "PATH_TO_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "cellId": "h3bpod5hrg5bb4zxwoz3r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/text_attacks/notebooks\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "cellId": "k9zyj6n878d3ys9is0gyku"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "# сохраним модель!\n",
    "\n",
    "torch.save(model.state_dict(), '../data/amzn.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "cellId": "bdk290bh2pl4dvi08xi9l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "#загрузим модель\n",
    "\n",
    "device = torch.device(\"cuda\") \n",
    "#model = TheModelClass(args, *kwargs) \n",
    "model.load_state_dict(torch.load('../data/amzn.bin')) \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "njbmlzhnuda604qntvqt96"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "cellId": "yo3hba3sjdcplp76ip3qv",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "68xreA9JAmG5",
    "outputId": "70b8500d-7efc-4c99-de1f-05e8795e6298"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "c6ihl02zkkd3klrsvlcosb",
    "id": "mkyubuJSOzg3"
   },
   "source": [
    "# 5. Performance On Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "de3s75v75tom1mpybi35ba",
    "id": "Tg42jJqqM68F"
   },
   "source": [
    "### 5.1. Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "pl9g4lgxyra35b2h2mkdzh",
    "id": "xWe0_JW21MyV"
   },
   "source": [
    "\n",
    "We'll need to apply all of the same steps that we did for the training data to prepare our test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "cellId": "tfbhrmflpbbihjfgwna4q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['birds', 'bunny rabbit central', 'cats', 'dogs',\n",
       "       'fish aquatic pets', 'small animals'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "cellId": "617uf5lddvio95rmwnnqdb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAN0LZBOOPVh",
    "outputId": "7385ca3f-72d5-45f0-bbfe-5056c2f62c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 17,353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2227: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = valid_df\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df.text.values\n",
    "labels = le.transform(df.label.values)\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.  # max lenght IS SET TO 64!\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 144    #BATCH SIZE!\n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "0kiqap2yr79rvokzx7rxt",
    "id": "16lctEOyNFik"
   },
   "source": [
    "## 5.2. Evaluate on Test Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "5828vkiyvxxmevpe3zc",
    "id": "rhR99IISNMg9"
   },
   "source": [
    "\n",
    "With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "cellId": "23vn9npnejkj0euf9yvqb2r8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hba10sXR7Xi6",
    "outputId": "e35f0a6e-72c5-4bd0-9c4b-dcec9ef5059d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 17,353 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions.\n",
    "      result = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     return_dict=True)\n",
    "\n",
    "  logits = result.logits\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "j6vmw7av6948rt95g50v7",
    "id": "1YrjAPX2V-l4"
   },
   "source": [
    "Now we'll combine the results for all of the batches and calculate our final MCC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "cellId": "vstmq7wnnkeke6e8yjjuob"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17353,), (17353,))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "flat_true_labels.shape, flat_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "cellId": "zz5wgkkxip1zo9r77flcx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCYZa1lQ8Jn8",
    "outputId": "b4650298-0e35-4ed8-be13-83f074a617ed"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "cellId": "z1yzrc16djfb9x7vhxcot"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "8lqa42u22akhmdby0521h6"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "print('accuracy', np.round(accuracy_score(flat_true_labels, flat_predictions),3)) #качество на тесте\n",
    "print('f1', np.round(f1_score(flat_true_labels, flat_predictions, average='macro'),2)) #качество на тесте\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "cellId": "97rsff4hgxiuqcx05v1ul9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dogs', 'dogs'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "le.inverse_transform([3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "cellId": "165zic2kyjlm7m5udpm6j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.829\n",
      "f1 0.69\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print('accuracy', np.round(accuracy_score(flat_true_labels, flat_predictions),3)) #качество на тесте\n",
    "print('f1', np.round(f1_score(flat_true_labels, flat_predictions, average='macro'),2)) #качество на тесте\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "cellId": "qrs82gyoegxcc9lgrmow8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.829\n",
      "f1 0.69\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print('accuracy', np.round(accuracy_score(flat_true_labels, flat_predictions),3)) #качество на тесте\n",
    "print('f1', np.round(f1_score(flat_true_labels, flat_predictions, average='macro'),2)) #качество на тесте\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "hvjlvn2n7qeqsldw60si3"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "мы видим, что модель обучалась в четыре раза дольше, тем не менее, качество получилось хуже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "cellId": "2dzzc1rsu4vmk5xfqhgypb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85       353\n",
      "           1       0.66      0.66      0.66       190\n",
      "           2       0.82      0.79      0.81      6166\n",
      "           3       0.86      0.89      0.88      9333\n",
      "           4       0.93      0.89      0.91      1197\n",
      "           5       0.54      0.48      0.51       114\n",
      "\n",
      "    accuracy                           0.85     17353\n",
      "   macro avg       0.78      0.76      0.77     17353\n",
      "weighted avg       0.85      0.85      0.85     17353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print(classification_report(flat_true_labels, flat_predictions)) #качество на тесте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "wx6epmkgtzn0lj9sclkiugh"
   },
   "source": [
    "## Проверим модель на робастность на основе адв примеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "vrifsyxtk2q9ttnspxy2f"
   },
   "source": [
    "#!g1.1\n",
    "\n",
    "### Релизуем рецепт - textfooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "mqae99jnqymxh5fymsuqmk"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "Почему он загружается целую вечность?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "jea5r6ns5ogsbxlpb8m4hs"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "import textattack\n",
    "model_wrapper = textattack.models.wrappers.HuggingFaceModelWrapper(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "yu1vixstynsh6yyext8c"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "attack = textattack.attack_recipes.TextFoolerJin2019.build(model_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "s3ek2py74noaco1v6fqfdi"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "attack_args = textattack.AttackArgs(num_examples=10, log_to_csv=\"log.csv\", checkpoint_interval=5, checkpoint_dir=\"checkpoints\", disable_stdout=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "cellId": "cvsuall10d6924ku9x5yc"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "valid_df['label'] = le.transform(valid_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "gxfuxx9i34cpvy26lwhhb9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "cellId": "cbuhcgna768fjh3pdt6rls"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9e27241b43415c9ba1072333eff7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /tmp/xdg_cache/huggingface/datasets/csv/default-6f749cb1edb1abeb/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
      "Dataset csv downloaded and prepared to /tmp/xdg_cache/huggingface/datasets/csv/default-6f749cb1edb1abeb/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6f749cb1edb1abeb\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/tqdm/std.py:1133: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  for obj in iterable:\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/tqdm/std.py:1133: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  for obj in iterable:\n",
      "Failed to deserialize variable 'dataset'. Run the following code to delete it:\n",
      "  del_datasphere_variables('dataset')\n",
      "Traceback (most recent call last):\n",
      "  File \"/kernel/lib/python3.7/site-packages/ml_kernel/state/state_protocol.py\", line 283, in _load_component\n",
      "    value = unpickler.load()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 564, in __setstate__\n",
      "    pa_table = reader._read_files([data_file])\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/datasets/arrow_reader.py\", line 171, in _read_files\n",
      "    pa_table: pa.Table = self._get_dataset_from_filename(f_dict, in_memory=in_memory)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/datasets/arrow_reader.py\", line 302, in _get_dataset_from_filename\n",
      "    pa_table = ArrowReader.read_table(filename, in_memory=in_memory)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/datasets/arrow_reader.py\", line 322, in read_table\n",
      "    stream = stream_from(filename)\n",
      "  File \"pyarrow/io.pxi\", line 841, in pyarrow.lib.memory_map\n",
      "  File \"pyarrow/io.pxi\", line 802, in pyarrow.lib.MemoryMappedFile._open\n",
      "  File \"pyarrow/error.pxi\", line 143, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 112, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/tmp/xdg_cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/imdb-test.arrow'. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "ml_kernel.state.state_protocol.KernelStateProtocol.DeserializationException: ['dataset']\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "#label_map\n",
    "from datasets import load_dataset\n",
    "dataset = Dataset.from_pandas(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "cellId": "riniy4b4fmizmj61x66u29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ml_kernel.state.state.LazyVariable at 0x7f6eff437f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "rwc5un4j6dapsge9ihad"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "attacker = textattack.Attacker(attack, dataset, attack_args)\n",
    "\n",
    "# что на вход можно подавать в качестве аргумента для датасета? туториал подсказывает, что нужно сначала загрузить датасет в библиотеку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "34jzj8vtsmxbjrbmai8gw8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "attacker.attack_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "1rqi6vx2rhkklmesotrqfl"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "dataset = textattack.datasets.HuggingFaceDataset(\"imdb\", split=\"test\")\n",
    ">>> attack = textattack.attack_recipes.TextFoolerJin2019.build(model_wrapper)\n",
    ">>> # Attack 20 samples with CSV logging and checkpoint saved every 5 interval\n",
    ">>> attack_args = textattack.AttackArgs(num_examples=20, log_to_csv=\"log.csv\", checkpoint_interval=5, checkpoint_dir=\"checkpoints\", disable_stdout=True)\n",
    ">>> attacker = textattack.Attacker(attack, dataset, attack_args)\n",
    ">>> attacker.attack_dataset()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT Fine-Tuning Sentence Classification v4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "58fc2266-af5f-4ac3-91de-b95feaf01c3c",
  "notebookPath": "text_attacks/notebooks/BERT_Amazon.ipynb",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0fe5b1d0540240a8a8426352c24b2887": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1058e0b5baa248faa60c1ad146d10bf7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1296a3d754b344a482a03e5af84e805e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8874fec8a404ae89a38fd2ecbb357cf",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2755b9838bae408ca8cf667ad9d501fc",
      "value": 433
     }
    },
    "1c2b0ede959142fc89bf07a9c88df638": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23ca9359e6c44232a1346e6f2ab7e48c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fe5b1d0540240a8a8426352c24b2887",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6c7dec7b1e804c2195f6e60fb3c1d18e",
      "value": 440473133
     }
    },
    "2755b9838bae408ca8cf667ad9d501fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "375cc635389c4ddb9bf2aa443df58bae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "440da34c72344cb08e4a1ee5de7049ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "472198d5b6a748b3a81f9364fd1fa711": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b1e27aff6f04fec8268d951e46b1e63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c7dec7b1e804c2195f6e60fb3c1d18e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6f132d7bb83d41b6847df0d0ec0a1b92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_978c24b18b594eaf8ca47730a88eefb9",
      "placeholder": "​",
      "style": "IPY_MODEL_a7bdbedc75de4f77b45f1389c2ea0abc",
      "value": " 433/433 [00:00&lt;00:00, 2.02kB/s]"
     }
    },
    "82ddfcea0e4c4e5a86cf6eca8585be8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8c76faadf2f4415393c6f0a805f0d72b",
       "IPY_MODEL_e0bb735fda99434a90380e7fc664212d"
      ],
      "layout": "IPY_MODEL_8a256ba4a19e4ec98fe3c3c99fba4daa"
     }
    },
    "8a256ba4a19e4ec98fe3c3c99fba4daa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c76faadf2f4415393c6f0a805f0d72b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1058e0b5baa248faa60c1ad146d10bf7",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cdb78e75309f4bc09366533331e72431",
      "value": 231508
     }
    },
    "978c24b18b594eaf8ca47730a88eefb9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7bdbedc75de4f77b45f1389c2ea0abc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf9dfa1ff3e642fbb74c5146d21044c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1296a3d754b344a482a03e5af84e805e",
       "IPY_MODEL_6f132d7bb83d41b6847df0d0ec0a1b92"
      ],
      "layout": "IPY_MODEL_1c2b0ede959142fc89bf07a9c88df638"
     }
    },
    "cdb78e75309f4bc09366533331e72431": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cea84f9c3db641acb98314028b305514": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d689bc8d488a4dc09c393b4fc9747bcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_440da34c72344cb08e4a1ee5de7049ee",
      "placeholder": "​",
      "style": "IPY_MODEL_4b1e27aff6f04fec8268d951e46b1e63",
      "value": " 440M/440M [00:07&lt;00:00, 55.5MB/s]"
     }
    },
    "e0bb735fda99434a90380e7fc664212d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_472198d5b6a748b3a81f9364fd1fa711",
      "placeholder": "​",
      "style": "IPY_MODEL_375cc635389c4ddb9bf2aa443df58bae",
      "value": " 232k/232k [00:00&lt;00:00, 616kB/s]"
     }
    },
    "f8874fec8a404ae89a38fd2ecbb357cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe254c3bcc08402eb506f0e98f5673a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_23ca9359e6c44232a1346e6f2ab7e48c",
       "IPY_MODEL_d689bc8d488a4dc09c393b4fc9747bcb"
      ],
      "layout": "IPY_MODEL_cea84f9c3db641acb98314028b305514"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
