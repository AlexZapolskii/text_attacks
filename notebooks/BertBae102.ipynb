{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "56cf80d7",
   "metadata": {
    "cellId": "54796vl6002ts48v9mw6wm"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8abda8",
   "metadata": {
    "cellId": "459hr5kz7rwuqrtjri316k"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "print('hello cudab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aad8e2",
   "metadata": {
    "cellId": "5barb4dy6lj1rnzxce8tcn"
   },
   "source": [
    "# Работа состоит из несольких частей\n",
    "    - код\n",
    "    - гипотезы\n",
    "    - текст\n",
    "    - обзор литературы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1096a06",
   "metadata": {
    "cellId": "cl8nhnpbrfzi92kgpqgfr"
   },
   "source": [
    "Мы хотим оценить влияние различных адверсариальных атак на современные NLP модели\n",
    "\n",
    "В рамках данного исследования мы остановимся на задаче классификации\n",
    "\n",
    "Следовательно: нам нужны\n",
    "    - Датасеты для классификации\n",
    "        - 2 на Английском (common domain, specific domain)\n",
    "        - 2 на Русском (common domain, specific domain)\n",
    "        \n",
    "    - Модели для классификации\n",
    "        - Мы можем использовать TF-IDF на log-reg\n",
    "        - Bert и его разновидности\n",
    "            - Английский Берт (Bert-base-uncased, distilled-bert-uncades)\n",
    "            - Мультиязычный Берт ?\n",
    "            - Русский Берт (DeepPavlov, дистилированная модель от Давида Деле)\n",
    "            \n",
    "    - Атаки:\n",
    "        - BAE\n",
    "        - TextFooler\n",
    "        - Другие атаки из модуля TextAtack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682f936",
   "metadata": {
    "cellId": "gu2rw9225rbiz3z3xadz19"
   },
   "source": [
    "Постановка эксперимента:\n",
    "    - Пока что не трогаем лог-рег\n",
    "    - Исследуем Берт\n",
    "    \n",
    "Задача1:\n",
    "    - fine-tuning Берта под задачу классификации\n",
    "    - Оценка качества на валидации\n",
    "    - Подготовка адверсариальных примеров на основе валидационного датасета\n",
    "    - Оценка качества на адверсариальных примерах"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e647e70",
   "metadata": {
    "cellId": "saakxkkmsbgh8vdvhyfv9"
   },
   "source": [
    "Этапы работы:\n",
    "    1) Загрузка и препроцессинг данных\n",
    "    2) Fine-tuning соответствующей модели\n",
    "    3) Валидация\n",
    "    4) Генерация адверсариальных примеров\n",
    "    5) Оценка качества\n",
    "        - Автоматическая валидация:\n",
    "            - accuracy\n",
    "            - semantic score\n",
    "        - Human evaluation\n",
    "            - Классификация примеров\n",
    "            - Оценка \"реалистичности и грамотности сгенерированных примеров\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a556c9e",
   "metadata": {
    "cellId": "z4hc8m4yopggtq9bgw80cn"
   },
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff60e23f",
   "metadata": {
    "cellId": "xnm305t8ekbac0hmqtqs"
   },
   "source": [
    "построим пайплайн на основе ноутбука https://www.kaggle.com/kashnitsky/distillbert-catalyst-amazon-product-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5affaea",
   "metadata": {
    "cellId": "0fzx6zll9zpmb3yh2ntq43n"
   },
   "source": [
    "%pip install -U catalyst transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51522ab",
   "metadata": {
    "cellId": "zx8qaw5dpfyt5f3vr9ow"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#%pip install -U transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cb8b6d",
   "metadata": {
    "cellId": "un7fzl863yxptu1n05zj"
   },
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/catalyst-team/catalyst@master --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fe17cb0",
   "metadata": {
    "cellId": "tlbnzbgdhem5c8r3uyjy68"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Python \n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "from typing import Mapping, List\n",
    "from pprint import pprint\n",
    "\n",
    "# Numpy and Pandas \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformers \n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "# Catalyst\n",
    "from catalyst.dl import SupervisedRunner\n",
    "from catalyst import dl  # импорт вместо catalyst.dl.callbacks\n",
    "\n",
    "#from catalyst.dl.callbacks import AccuracyCallback, F1ScoreCallback, OptimizerCallback\n",
    "#from catalyst.dl.callbacks import CheckpointCallback, InferCallback\n",
    "from catalyst.utils import set_global_seed, prepare_cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e87b0c3e",
   "metadata": {
    "cellId": "te6zgj640xibz5syst30or"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "MODEL_NAME = 'distilbert-base-uncased' # pretrained model from Transformers\n",
    "LOG_DIR = \"./logdir_amazon_reviews\"    # for training logs and tensorboard visualizations\n",
    "NUM_EPOCHS = 3                         # smth around 2-6 epochs is typically fine when finetuning transformers\n",
    "BATCH_SIZE = 72                        # depends on your available GPU memory (in combination with max seq length)\n",
    "MAX_SEQ_LENGTH = 256                   # depends on your available GPU memory (in combination with batch size)\n",
    "LEARN_RATE = 5e-5                      # learning rate is typically ~1e-5 for transformers\n",
    "ACCUM_STEPS = 4                        # one optimization step for that many backward passes\n",
    "SEED = 17                              # random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d397061d",
   "metadata": {
    "cellId": "fd7cyh32q6km5krexy3p"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f042db0",
   "metadata": {
    "cellId": "y8z39qqsf2uf2i2dwb93"
   },
   "source": [
    "\n",
    "Amazon product reviews - competition. Given text of a review, we need to classify it into one of 6 categories: dogs, cats, fish aquatic pets, birds, and two others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5fd5d4",
   "metadata": {
    "cellId": "6uruu1o38q5q3xjhcr808p"
   },
   "outputs": [],
   "source": [
    "#!unzip data/amazon-pet-product-reviews-classification.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b2d9416",
   "metadata": {
    "cellId": "74v18vgr8bsxh2ul5jhcc"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# to reproduce, download the data and customize this path\n",
    "PATH_TO_DATA = 'data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b92e4e09",
   "metadata": {
    "cellId": "1prtmddjoia6f5cyapdll6"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "#загрузим данные из csv\n",
    "train_df = pd.read_csv(PATH_TO_DATA + 'train.csv', index_col='id').fillna('')\n",
    "valid_df = pd.read_csv(PATH_TO_DATA + 'valid.csv', index_col='id').fillna('')\n",
    "test_df = pd.read_csv(PATH_TO_DATA + 'test.csv', index_col='id').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d477ef1",
   "metadata": {
    "cellId": "ydc7ziu6b2hmzaenvcbl6"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be87eed8",
   "metadata": {
    "cellId": "q0by0zhhx3czumv4mxmx7j"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "52к наблюдений в трейне, 17к в валидации, 17 к в тесте, в тесте - нет меток класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53d45b",
   "metadata": {
    "cellId": "fptospdqf8ght95hnewr0e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "Идея - в нашем случае - можем проверить результат модели как на валидации, так и отправкой сабмита на кээгл с adv примерами (результатами предсказаний)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a298b6b8",
   "metadata": {
    "cellId": "11julyzvcloc5098xdvmvf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b12af70a",
   "metadata": {
    "cellId": "khx4umt3zi7zkh19l5rqsj"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_df.text[4]\n",
    "# явно отзыв по собакам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4664612b",
   "metadata": {
    "cellId": "5nu4l4wtdromryfwhx0wkp"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# target distribution\n",
    "train_df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce39046b",
   "metadata": {
    "cellId": "mj6ki5shtwpw4fqryvrpp"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# statistics of text length (in words)\n",
    "train_df['text'].apply(lambda s: len(s.split())).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5db5ce4",
   "metadata": {
    "cellId": "l01o9p40c2go8pcbzuew1"
   },
   "source": [
    "## Torch Dataset\n",
    "This is left for user to be defined. Catalyst will take care of the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94fcb02b",
   "metadata": {
    "cellId": "q9jkyhmmz7e7scmvlpq3"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "# нужно разобраться, как под другой датасет переписать класс\n",
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wrapper around Torch Dataset to perform text classification\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 texts: List[str],\n",
    "                 labels: List[str] = None,\n",
    "                 label_dict: Mapping[str, int] = None,\n",
    "                 max_seq_length: int = 512,  #ограничение берта на длину последовательности в 512 токенов\n",
    "                 model_name: str = 'distilbert-base-uncased'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (List[str]): a list with texts to classify or to train the\n",
    "                classifier on\n",
    "            labels List[str]: a list with classification labels (optional)\n",
    "            label_dict (dict): a dictionary mapping class names to class ids,\n",
    "                to be passed to the validation data (optional)\n",
    "            max_seq_length (int): maximal sequence length in tokens,\n",
    "                texts will be stripped to this length\n",
    "            model_name (str): transformer model name, needed to perform\n",
    "                appropriate tokenization\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.label_dict = label_dict\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        if self.label_dict is None and labels is not None:\n",
    "            # {'class1': 0, 'class2': 1, 'class3': 2, ...}\n",
    "            # using this instead of `sklearn.preprocessing.LabelEncoder`\n",
    "            # no easily handle unknown target values\n",
    "            self.label_dict = dict(zip(sorted(set(labels)),\n",
    "                                       range(len(set(labels)))))\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # suppresses tokenizer warnings\n",
    "        logging.getLogger(\n",
    "            \"transformers.tokenization_utils\").setLevel(logging.FATAL)\n",
    "\n",
    "        # special tokens for transformers\n",
    "        # in the simplest case a [CLS] token is added in the beginning\n",
    "        # and [SEP] token is added in the end of a piece of text\n",
    "        # [CLS] <indexes text tokens> [SEP] .. <[PAD]>\n",
    "        self.sep_vid = self.tokenizer.vocab[\"[SEP]\"]\n",
    "        self.cls_vid = self.tokenizer.vocab[\"[CLS]\"]\n",
    "        self.pad_vid = self.tokenizer.vocab[\"[PAD]\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index) -> Mapping[str, torch.Tensor]:\n",
    "        \"\"\"Gets element of the dataset\n",
    "\n",
    "        Args:\n",
    "            index (int): index of the element in the dataset\n",
    "        Returns:\n",
    "            Single element by index\n",
    "        \"\"\"\n",
    "\n",
    "        # encoding the text\n",
    "        x = self.texts[index]\n",
    "        x_encoded = self.tokenizer.encode(\n",
    "            x,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_seq_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).squeeze(0)\n",
    "\n",
    "        # padding short texts\n",
    "        true_seq_length = x_encoded.size(0)\n",
    "        pad_size = self.max_seq_length - true_seq_length\n",
    "        pad_ids = torch.Tensor([self.pad_vid] * pad_size).long()\n",
    "        x_tensor = torch.cat((x_encoded, pad_ids))\n",
    "\n",
    "        # dealing with attention masks - there's a 1 for each input token and\n",
    "        # if the sequence is shorter that `max_seq_length` then the rest is\n",
    "        # padded with zeroes. Attention mask will be passed to the model in\n",
    "        # order to compute attention scores only with input data\n",
    "        # ignoring padding\n",
    "        mask = torch.ones_like(x_encoded, dtype=torch.int8)\n",
    "        mask_pad = torch.zeros_like(pad_ids, dtype=torch.int8)\n",
    "        mask = torch.cat((mask, mask_pad))\n",
    "\n",
    "        output_dict = {\n",
    "            \"features\": x_tensor,\n",
    "            'attention_mask': mask\n",
    "        }\n",
    "\n",
    "        # encoding target\n",
    "        if self.labels is not None:\n",
    "            y = self.labels[index]\n",
    "            y_encoded = torch.Tensor(\n",
    "                [self.label_dict.get(y, -1)]\n",
    "            ).long().squeeze(0)\n",
    "            output_dict[\"targets\"] = y_encoded\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1637f9",
   "metadata": {
    "cellId": "qfdor4l7w9q8m11y9h08f5"
   },
   "source": [
    "Create Torch Datasets with train, validation, and test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d018aa1",
   "metadata": {
    "cellId": "d4951pjwhvozr936kan3h"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_dataset = TextClassificationDataset(\n",
    "    texts=train_df['text'].values.tolist(),\n",
    "    labels=train_df['label'].values.tolist(),\n",
    "    label_dict=None,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "valid_dataset = TextClassificationDataset(\n",
    "    texts=valid_df['text'].values.tolist(),\n",
    "    labels=valid_df['label'].values.tolist(),\n",
    "    label_dict=train_dataset.label_dict,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "test_dataset = TextClassificationDataset(\n",
    "    texts=test_df['text'].values.tolist(),\n",
    "    labels=None,\n",
    "    label_dict=None,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    model_name=MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a7193",
   "metadata": {
    "cellId": "sowykc4ff5m3g5cu8jtzyj"
   },
   "source": [
    "We infer the number of classes from the training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78ee99db",
   "metadata": {
    "cellId": "x8ny44bmhxr4zoj1su42ul"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "NUM_CLASSES = len(train_dataset.label_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d731e176",
   "metadata": {
    "cellId": "r5arjk3yfjg54wamvgto6"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_df.loc[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1102e3e",
   "metadata": {
    "cellId": "3vpp8g1avi3zelcudeket"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#мы видим, что в train_df - только описание и лейбл\n",
    "# трайн датасет - выплевывает обработанную последовательность в виде словаря\n",
    "#  attention_mask - 1 для токенов, 0 - для pad\n",
    "# 'features' - это индексы токенов из словаря 101 - CLS, и так далее\n",
    "# targets - метка класса (лейбл)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27064df8",
   "metadata": {
    "cellId": "yqifo4t9cfh2wpbq2ekn66"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "pprint(train_dataset[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984736a5",
   "metadata": {
    "cellId": "43mxdi60qt45yc1c443p29"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# последовательность - Датасет => Даталоадер (видим, что в трейн даталоадаре и трейн и валидация)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6144ce37",
   "metadata": {
    "cellId": "fy07kzfs1b5cwo4qw017o"
   },
   "source": [
    "Finally, we define standard PyTorch loaders. This dictionary will be fed to Catalyst.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc3d4084",
   "metadata": {
    "cellId": "26mq9xdwzw7na1e34bckhe"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_val_loaders = {\n",
    "    \"train\": DataLoader(dataset=train_dataset,\n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=True),\n",
    "    \"valid\": DataLoader(dataset=valid_dataset,\n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=False)    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e380c",
   "metadata": {
    "cellId": "fsghxlzusgr0g88z91twlk"
   },
   "source": [
    "## The model¶\n",
    "It's going to be a slightly simplified version of DistilBertForSequenceClassification by HuggingFace.<br> We need only predicted probabilities as output, nothing more - we don't need neither loss to be output nor hidden states or attentions (as in the original implementation).\n",
    "\n",
    "A good overview of DistilBERT is done in this great post by Jay Alammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10968b3a",
   "metadata": {
    "cellId": "b3tfsy304to68o31c2cn26"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class DistilBertForSequenceClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified version of the same class by HuggingFace.\n",
    "    See transformers/modeling_distilbert.py in the transformers repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model_name: str, num_classes: int = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pretrained_model_name (str): HuggingFace model name.\n",
    "                See transformers/modeling_auto.py\n",
    "            num_classes (int): the number of class labels\n",
    "                in the classification task\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            pretrained_model_name, num_labels=num_classes)\n",
    "\n",
    "        self.distilbert = AutoModel.from_pretrained(pretrained_model_name,\n",
    "                                                    config=config)\n",
    "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
    "        self.classifier = nn.Linear(config.dim, num_classes)\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "\n",
    "    def forward(self, features, attention_mask=None, head_mask=None):\n",
    "        \"\"\"Compute class probabilities for the input sequence.\n",
    "\n",
    "        Args:\n",
    "            features (torch.Tensor): ids of each token,\n",
    "                size ([bs, seq_length]\n",
    "            attention_mask (torch.Tensor): binary tensor, used to select\n",
    "                tokens which are used to compute attention scores\n",
    "                in the self-attention heads, size [bs, seq_length]\n",
    "            head_mask (torch.Tensor): 1.0 in head_mask indicates that\n",
    "                we keep the head, size: [num_heads]\n",
    "                or [num_hidden_layers x num_heads]\n",
    "        Returns:\n",
    "            PyTorch Tensor with predicted class probabilities\n",
    "        \"\"\"\n",
    "        assert attention_mask is not None, \"attention mask is none\"\n",
    "        distilbert_output = self.distilbert(input_ids=features,\n",
    "                                            attention_mask=attention_mask,\n",
    "                                            head_mask=head_mask)\n",
    "        # we only need the hidden state here and don't need\n",
    "        # transformer output, so index 0\n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        # we take embeddings from the [CLS] token, so again index 0\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)  # (bs, dim)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af57a607",
   "metadata": {
    "cellId": "kn5snsjlpigevto9bopm3"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model = DistilBertForSequenceClassification(pretrained_model_name=MODEL_NAME,\n",
    "                                            num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad020836",
   "metadata": {
    "cellId": "uagcx6gelxl0n3y088hma5h"
   },
   "source": [
    "Model training\n",
    "First we specify optimizer and scheduler (pure PyTorch). Then Catalyst stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a68604",
   "metadata": {
    "cellId": "ufe3qc49bddcx2fh6dqdf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#как правильно задать шедулер?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65793350",
   "metadata": {
    "cellId": "slyajk64ikb8vg2crfkwoh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARN_RATE)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "#заменим шидулер,  чтобы не ругался:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c617aa",
   "metadata": {
    "cellId": "69tzcd7jbakpb4hogcq94"
   },
   "source": [
    "To run Deep Learning experiments, Catalyst resorts to the Runner abstraction, in particular, to SupervisedRunner.\n",
    "\n",
    "SupervisedRunner implements the following methods:\n",
    "\n",
    "train - starts the training process of the model\n",
    "predict_loader - makes a prediction on the whole loader with the specified model\n",
    "infer - makes the inference on the model\n",
    "To train the model within this interface you pass the following to the train method:\n",
    "\n",
    "model (torch.nn.Module) – PyTorch model to train\n",
    "criterion (nn.Module) – PyTorch criterion function for training\n",
    "optimizer (optim.Optimizer) – PyTorch optimizer for training\n",
    "loaders (dict) – dictionary containing one or several torch.utils.data.DataLoader for training and validation\n",
    "logdir (str) – path to output directory. There Catalyst will write logs, will dump the best model and the actual code to train the model\n",
    "callbacks – list of Catalyst callbacks\n",
    "scheduler (optim.lr_scheduler._LRScheduler) – PyTorch scheduler for training\n",
    "...\n",
    "In our case we'll pass the created DistilBertForSequenceClassification model, cross-entropy criterion, Adam optimizer, scheduler and data loaders that we created earlier. Also, we'll be tracking accuracy and thus will need AccuracyCallback. To perform batch accumulation, we'll be using OptimizationCallback.\n",
    "\n",
    "There are many more useful callbacks implemented, also check out Catalyst examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79089fe9",
   "metadata": {
    "cellId": "qjkd44yu6xica6mnfgg4yv"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"    # can be changed in case of multiple GPUs onboard\n",
    "set_global_seed(SEED)                       # reproducibility\n",
    "prepare_cudnn(deterministic=True)           # reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1e4d4",
   "metadata": {
    "cellId": "ocsy5krhsygflxh5x2hrv"
   },
   "source": [
    "!!На этом замечательно моменте все сломалось - из за того, что не смогли заимпортить\n",
    "\n",
    "from catalyst.dl.callbacks import AccuracyCallback, F1ScoreCallback, OptimizerCallback\n",
    "from catalyst.dl.callbacks import CheckpointCallback, InferCallback\n",
    "\n",
    "результат - ModuleNotFoundError: No module named 'catalyst.dl.callbacks'\n",
    "видимо, код изменился настолько, что с этой частью нужно разбираться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "33c30c60",
   "metadata": {
    "cellId": "mzgwzqt70fy5138fn98fg"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import catalyst\n",
    "from catalyst import dl, metrics, utils\n",
    "catalyst.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3c650b8",
   "metadata": {
    "cellId": "vxsknkm5n2j8ee7tja5nwm"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from catalyst.dl import AccuracyCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37083f95",
   "metadata": {
    "cellId": "v2se33w75obx8mmftxkj4"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "ВОПРОС: и все таки, куда пропали callbacks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2601154",
   "metadata": {
    "cellId": "yxvst3jw5g0zbdkmqcqa1f"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "class BertRunner(SupervisedRunner):\n",
    "    def _handle_batch(self, batch):\n",
    "        self.input = batch\n",
    "        self.output = self.model(**{k: batch[k] for k in self.input_key}, return_dict=True)\n",
    "\n",
    "\n",
    "runner = BertRunner(input_key=[\"features\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73abaf7c",
   "metadata": {
    "cellId": "bwutm97tgv46r3cdg2sgtr"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "начал выполняться в 3-13  закончил в 3-35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "301bbcbb",
   "metadata": {
    "cellId": "8fd1v701ggwe3rhu93lwmt"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "\n",
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    loaders=train_val_loaders,\n",
    "    logdir=LOG_DIR,\n",
    "    num_epochs=3,\n",
    "    verbose=True,\n",
    "    callbacks=[AccuracyCallback(input_key=\"logits\", target_key=\"targets\", num_classes=NUM_CLASSES)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab3bba",
   "metadata": {
    "cellId": "f12i1uxc2n4mrl0lz3jjjo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed217d17",
   "metadata": {
    "cellId": "aq3a3wfdpu5j6phu9djd49"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "\n",
    "pred1 = next(runner.predict_loader(loader=test_loaders[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05dd9aea",
   "metadata": {
    "cellId": "m6vxb2k8euc7p7w1mqokm"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "pred1['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0f7c5",
   "metadata": {
    "cellId": "fd6v20pfa0j1grp6zs4dmr"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "пока что мы получили лишь 72, должны - 17353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01a28b1b",
   "metadata": {
    "cellId": "2royg4rl4o4pdvd9ibfris"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "pred2 = [p['logits'].cpu().numpy().argmax() for p in runner.predict_loader(loader=test_loaders[\"test\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4d866f69",
   "metadata": {
    "cellId": "poampgqqfvembnym2k88m"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "type(pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c4fb5487",
   "metadata": {
    "cellId": "bfwji2b1vskyaacucwnldq"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "len((pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "881e24c0",
   "metadata": {
    "cellId": "28bao0bh3vh754rqqgchf2"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "type(pred2[0]), pred2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ffac877d",
   "metadata": {
    "cellId": "xd1etpull9y1r88zk540d"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "pred2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f5038b60",
   "metadata": {
    "cellId": "dvq90ibrm7uju21ztrte0q"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "from catalyst.dl.utils import plot_metrics\n",
    "\n",
    "utils.plot_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e7f07e",
   "metadata": {
    "cellId": "gt7v9hhh46ksmx09egii5c"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34124fe",
   "metadata": {
    "cellId": "mj8egmyfkvnxf3j0cwz9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "\n",
    "# plot_metrics(\n",
    "#     logdir=LOG_DIR,\n",
    "#     step='batch',\n",
    "#     metrics=['accuracy01']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a474c11",
   "metadata": {
    "cellId": "srt4z96fljjosvhorzcvs"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634dfd3",
   "metadata": {
    "cellId": "qpc8inltxwna0f7vj7sw"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb7bf6",
   "metadata": {
    "cellId": "jkke4805rckotl2pnpkqes"
   },
   "source": [
    "## Inference for the test set\n",
    "Let's create a Torch loader for the test set and launch infer to actually make predictions fot the test set.<br> First, we load the best model checkpoint, then make inference with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99824268",
   "metadata": {
    "cellId": "haa05fxsenqkj2sisjk6"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "test_loaders = {\n",
    "    \"test\": DataLoader(dataset=test_dataset,\n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=False) \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4b86fddf",
   "metadata": {
    "cellId": "z2mbgxc2gsc6pktgyoatk"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "features_batch = next(iter(test_loaders[\"test\"]))\n",
    "prediction_batch = runner.predict_batch(features_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "696fd0bd",
   "metadata": {
    "cellId": "emtaeyfnou986vvcwl1pb"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "prediction_batch['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1f560a3f",
   "metadata": {
    "cellId": "zj2lcw7xrh4tlni1t4f9i"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ac5d9",
   "metadata": {
    "cellId": "24vb7w6ntwcztyvoxkn9w9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#похоже метод infer так же безнадежно устарел()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3823e216",
   "metadata": {
    "cellId": "r0896l0y38eon9s4jmb7w"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "runner.predict_loader(test_loaders, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a9017413",
   "metadata": {
    "cellId": "57xaxdazxn8opz19mpvt9l"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "for prediction in runner.predict_loader(loader=valid_dataset):\n",
    "    assert prediction.detach().cpu().numpy().shape[-1] == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b94bbc4",
   "metadata": {
    "cellId": "sry4okqs5s02dgtuoc6qlt"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "# model batch inference\n",
    "features_batch = next(iter(loaders[\"valid\"]))[0]\n",
    "prediction_batch = runner.predict_batch(features_batch)\n",
    "# model loader inference\n",
    "for prediction in runner.predict_loader(loader=loaders[\"valid\"]):\n",
    "    assert prediction.detach().cpu().numpy().shape[-1] == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ec7eb",
   "metadata": {
    "cellId": "u9dex1wkek408u4vtv4qa"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "predicted_probs = runner.callbacks[0].predictions['logits']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce06e2c",
   "metadata": {
    "cellId": "06s3vyet9xgk92z0wj91d4w"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "sample_sub_df = pd.read_csv(PATH_TO_DATA + 'sample_submission.csv',\n",
    "                           index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a80771",
   "metadata": {
    "cellId": "2l9rjizcrjxa9jmykmzn4"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_dataset.label_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199994e",
   "metadata": {
    "cellId": "zvizc7o3efj27d89wg0al"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "sample_sub_df['label'] = predicted_probs.argmax(axis=1)\n",
    "sample_sub_df['label'] = sample_sub_df['label'].map({v:k for k, v in train_dataset.label_dict.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aedbb6",
   "metadata": {
    "cellId": "ve1mh0gs60nck3q9c8vugs"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "sample_sub_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb0a762",
   "metadata": {
    "cellId": "m2840ruhipkjkb9xtsak"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "sample_sub_df.to_csv('distillbert_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "description": "IPython kernel implementation for Yandex DataSphere",
   "display_name": "Yandex DataSphere Kernel",
   "name": "python3",
   "resources": {},
   "spec": {
    "argv": [
     "/bin/true"
    ],
    "codemirror_mode": "python",
    "display_name": "Yandex DataSphere Kernel",
    "env": {},
    "help_links": [],
    "language": "python"
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "fcbde092-ab1e-471d-a905-bfb3fcb010a3",
  "notebookPath": "text_attacks/notebooks/BertBae102.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
