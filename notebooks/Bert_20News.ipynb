{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945fd410",
   "metadata": {
    "cellId": "0qxqnxov5zirs9zfoel9y"
   },
   "source": [
    "#!g1.1\n",
    "\n",
    "Загрузим датасет 20NewsGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cf1fb55",
   "metadata": {
    "cellId": "19mal7l0xbnz43hvgsd6er",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oYsV4H8fCpZ-",
    "outputId": "2e60467d-6a6b-4898-e2f4-f42884cc6092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfb14e96",
   "metadata": {
    "cellId": "2hdnmhzyrkycihd6i81rlo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "#загрузим обучающую выборку\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36972dd8",
   "metadata": {
    "cellId": "5waulwu83c7oc3mpf4bnu"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "X = newsgroups_train.data\n",
    "y = newsgroups_train.target\n",
    "\n",
    "X = pd.DataFrame(X)\n",
    "X.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358eaf7e",
   "metadata": {
    "cellId": "w0s3xyhmlzalq9dual17ss"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47874361",
   "metadata": {
    "cellId": "pbj9xlz29k5dzvjuqxpwj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4, 4, ..., 3, 1, 8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9efed24c",
   "metadata": {
    "cellId": "cocgvjx60p4ruwwhrqdxka"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11314, 1), (11314,))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4531fb2b",
   "metadata": {
    "cellId": "8zs5ncx93hungbl327yeb"
   },
   "source": [
    "#!g1.1\n",
    "всего 11к записей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "044727eb",
   "metadata": {
    "cellId": "m3pyc9dvynkcrd5nje4aaj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: jae2001@andy.bgsu.edu (Jason Ehas)\n",
      "Subject: Re: Giveaways\n",
      "Organization: Home of 1984 NCAA hockey champs\n",
      "Lines: 43\n",
      "\n",
      "In article <1qi44l$kqr@access.digex.net>, steveg@cadkey.com (Steve\n",
      "Gallichio) wrote:\n",
      "> \n",
      "> \n",
      "> John P. Curcio (jpc@philabs.philips.com) responded to my drivel:\n",
      "> \n",
      "> >steveg@cadkey.com (Steve Gallichio) writes:\n",
      "> > \n",
      "> >>I still am surprised that no one has tried giving away the goodies at the end\n",
      "> >>of the game. The two problems with that, of course, are that you would want\n",
      "> >>to make sure the first people in the building would be assured of getting\n",
      "> >>them (probably redeemable vouchers), and that the building managers want to\n",
      "> >>avoid at all costs delaying people as they leave the building, if, for\n",
      "> >>instance, the goodies are given to people as they exit.\n",
      "> >\n",
      "> >I went to the New Jersey Devils/Carvel Ice Cream Puck Night (tm) last year to\n",
      "> >see the beloved Bruins play.  The pucks were given out at the end of the game.\n",
      "> >I could just imagine what would have happened late in the third if the Bruins\n",
      "> >were winning....\n",
      "> \n",
      "> It figures, after I posted the first article, I found out that the Whalers are\n",
      "> going to be using coupons for the the giveaway on Friday Night. I believe that\n",
      "> is is the \"Some Big Corporation (Probably a Bank) Flying Disk Night.\" I think\n",
      "> that we could all see the potential for danger here...\n",
      "> \n",
      "> >|> All in all, I have seen a whole bunch of giveaways land on the ice, and it\n",
      "> >|> never ceases to amuse me. I'm just thankful for the players that no one has\n",
      "> >|> yet to sponsor 'Lead Pipe Night' at any arenas...\n",
      "> >\n",
      "> >That's probably because they couldn't find anyone to sponser it... Maybe USS\n",
      "> >could sponser the Pittsburgh Penguins/US Steel Steel Rod Night-- close enough?\n",
      "> \n",
      "> Naah, it'd probably bounce off of Jay Caufield.\n",
      "> \n",
      "> -SG\n",
      "\n",
      "I was at a Cincinnati Cyclones game a year ago when the local country\n",
      "station sponsored a kazoo giveaway.  After a particularly bad call by the\n",
      "underexperienced ECHL ref, it was Kazoostorm time down on the ice.  I\n",
      "thought this was a pathetic display by the fans, but they were rightfully\n",
      "unhappy.\n",
      "\n",
      "Jason\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "#пример текста \n",
    "\n",
    "print(X.text[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9ed4a75",
   "metadata": {
    "cellId": "tjdpcgctiznzex56lqktc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rec.sport.hockey'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "#соответствующая категория:\n",
    "newsgroups_train.target_names[y[300]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f26d4eec",
   "metadata": {
    "cellId": "0cawyzkrt959lsmodvp5bl"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77c669fc",
   "metadata": {
    "cellId": "9ac8fr7owipuwknowuii5"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "#разобьем на трейн/тест \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  stratify = y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f426010",
   "metadata": {
    "cellId": "keu1ml56uicys13cxq5k2l",
    "id": "GuE5BqICAne2"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Get the lists of sentences and their labels.\n",
    "sentences = X_train.text.values\n",
    "labels = y_train\n",
    "\n",
    "#valid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51cc93",
   "metadata": {
    "cellId": "b3x2hw6jdnpr9sd619rzf"
   },
   "source": [
    "берем модель дистилированного Берта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f090617a",
   "metadata": {
    "cellId": "2pm5e95vz3nrb0phg6w7a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "82ddfcea0e4c4e5a86cf6eca8585be8d",
      "8a256ba4a19e4ec98fe3c3c99fba4daa",
      "8c76faadf2f4415393c6f0a805f0d72b",
      "e0bb735fda99434a90380e7fc664212d",
      "cdb78e75309f4bc09366533331e72431",
      "1058e0b5baa248faa60c1ad146d10bf7",
      "375cc635389c4ddb9bf2aa443df58bae",
      "472198d5b6a748b3a81f9364fd1fa711"
     ]
    },
    "id": "Z474sSC6oe7A",
    "outputId": "4e6d97b6-2d4c-42ca-c201-d2b4a88895b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# попробуем облегченную модель \n",
    "# Load the BERT tokenizer. distilbert-base-uncased\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)  #стандартная модель - bert-base-uncased, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccdaafc",
   "metadata": {
    "cellId": "kkl2ccwys2qx5qcukviq"
   },
   "source": [
    "#!g1.1\n",
    "## Оценим распределение длины текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c239eb77",
   "metadata": {
    "cellId": "lq8kc1e4u0945765308h99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  52947\n",
      "CPU times: user 1min 3s, sys: 92 ms, total: 1min 3s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%%time\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb1b1d",
   "metadata": {
    "cellId": "un00qynlmxgf0nfs6e3smk"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    " мы видим - что макс длина последоательности 52к "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4d09ff3",
   "metadata": {
    "cellId": "hgxeki50haqf55v3te4wdn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     7580.000000\n",
       "mean       287.968734\n",
       "std        529.736149\n",
       "min         17.000000\n",
       "25%        108.000000\n",
       "50%        177.000000\n",
       "75%        294.000000\n",
       "max      11278.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "# тем не менее, важно понимать распределение кол-ва токенов в последовательности:\n",
    "\n",
    "# statistics of text length (in words)\n",
    "X_train['text'].apply(lambda s: len(s.split())).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1634bcc9",
   "metadata": {
    "cellId": "aac00glhe24r2lwk7dceta"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='text'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "# тем не менее, важно понимать распределение кол-ва токенов в последовательности:\n",
    "import seaborn as sns\n",
    "# statistics of text length (in words)\n",
    "sns.boxplot(X_train['text'].apply(lambda s: len(s.split())), showfliers=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4d78d6f",
   "metadata": {
    "cellId": "bnn7mbekw5fotg939mejzg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769.0\n",
      "2004.42\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print(X_train['text'].apply(lambda s: len(s.split())).quantile(0.95))\n",
    "print(X_train['text'].apply(lambda s: len(s.split())).quantile(0.99))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e374f88",
   "metadata": {
    "cellId": "05p3gc8eu7gio490c2t1unb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512.0\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print(X_train['text'].apply(lambda s: len(s.split())).quantile(0.90))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c16f7b4",
   "metadata": {
    "cellId": "8muy236wn044petkltfwip"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "` мы видим, что 90% данных укладываются в стандартную длину последовательности для Берта"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3171b95a",
   "metadata": {
    "cellId": "5rjdfwipuead7vctxan0a"
   },
   "source": [
    "#!g1.1\n",
    "## Токенизируем "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acff326",
   "metadata": {
    "cellId": "i2b3l4sb9fg6rmeek1qvo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "выбрали длину последовательности - равной 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a4c0c6b",
   "metadata": {
    "cellId": "kzvwp3us6uby9ot9rxc2aa"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "#!g1.1\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Pad & truncate all sentences.   Установили на основе анализа длины текстов\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "#print('Original: ', sentences[0])\n",
    "#print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5027d0",
   "metadata": {
    "cellId": "36z03wsoqoy7m58vl4pu7e"
   },
   "source": [
    "#!g1.1\n",
    "\n",
    "## Training & Validation Split¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e1fd2d0",
   "metadata": {
    "cellId": "226c3inrsg27jflubimdgb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,002 training samples\n",
      "2,578 validation samples\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "#!g1.1\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.66 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84ccc99c",
   "metadata": {
    "cellId": "x6k3kv9qhieelymfpw45r",
    "id": "XGUqOCtgqGhP"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 64  # посмотрим, как зайдет по памяти 64   ВАЖНО!  - нужно задавать гиперпараметры - как аргументы\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d461276c",
   "metadata": {
    "cellId": "y784wg5asiwb5f1ykq8hf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 27 08:47:58 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:8C:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    35W / 300W |  29803MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     15154      C   python3                         29799MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284d1abb",
   "metadata": {
    "cellId": "d4mez6xqjwc5gxjat9s0g4"
   },
   "source": [
    "#!g1.1\n",
    "Загрузим модель и укажем кол-во классов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45bd12d8",
   "metadata": {
    "cellId": "1zeh7dezfb6vmvk6f2vuic"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "num_classes = pd.Series(y).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f43b85ad",
   "metadata": {
    "cellId": "hgav6slj0sbgbyp0owa45e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d17f4c90",
   "metadata": {
    "cellId": "7qbhxotsnage1gtfqbcxtj",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "bf9dfa1ff3e642fbb74c5146d21044c2",
      "1c2b0ede959142fc89bf07a9c88df638",
      "1296a3d754b344a482a03e5af84e805e",
      "6f132d7bb83d41b6847df0d0ec0a1b92",
      "2755b9838bae408ca8cf667ad9d501fc",
      "f8874fec8a404ae89a38fd2ecbb357cf",
      "a7bdbedc75de4f77b45f1389c2ea0abc",
      "978c24b18b594eaf8ca47730a88eefb9",
      "fe254c3bcc08402eb506f0e98f5673a7",
      "cea84f9c3db641acb98314028b305514",
      "23ca9359e6c44232a1346e6f2ab7e48c",
      "d689bc8d488a4dc09c393b4fc9747bcb",
      "6c7dec7b1e804c2195f6e60fb3c1d18e",
      "0fe5b1d0540240a8a8426352c24b2887",
      "4b1e27aff6f04fec8268d951e46b1e63",
      "440da34c72344cb08e4a1ee5de7049ee"
     ]
    },
    "id": "gFsCTp_mporB",
    "outputId": "af690f33-6cd5-4678-bdaf-209f068f70f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'vocab_projector.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'classifier.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'classifier.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "#НУЖНО УКАЗАТЬ КОЛ-ВО КЛАССОВ\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab. используем дистилированный берт\n",
    "    num_labels = num_classes, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970ac33b",
   "metadata": {
    "cellId": "uusr3geayukrxr9qj5pep8"
   },
   "source": [
    "#!g1.1\n",
    "\n",
    "# объявим вспомогателные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d333c934",
   "metadata": {
    "cellId": "mss9oxaqailcioqmms7nha",
    "id": "9cQNvaZ9bnyy"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "#!g1.1\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4    # Кол-во эпох!\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "#!g1.1\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd312c5",
   "metadata": {
    "cellId": "txjqspcs7fminq3obovui"
   },
   "source": [
    "## Запускаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b09383d3",
   "metadata": {
    "cellId": "cem7tki9y4fxbmmubgvi1d",
    "id": "gpt6tR83keZD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     79.    Elapsed: 0:00:34.\n",
      "\n",
      "  Average training loss: 3.02\n",
      "  Training epcoh took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.05\n",
      "  Validation Loss: 3.01\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     79.    Elapsed: 0:00:34.\n",
      "\n",
      "  Average training loss: 3.00\n",
      "  Training epcoh took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.08\n",
      "  Validation Loss: 2.97\n",
      "  Validation took: 0:00:12\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     79.    Elapsed: 0:00:34.\n",
      "\n",
      "  Average training loss: 2.92\n",
      "  Training epcoh took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.10\n",
      "  Validation Loss: 2.88\n",
      "  Validation took: 0:00:12\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     79.    Elapsed: 0:00:35.\n",
      "\n",
      "  Average training loss: 2.85\n",
      "  Training epcoh took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.13\n",
      "  Validation Loss: 2.82\n",
      "  Validation took: 0:00:12\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:05:15 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "#!g1.1\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
    "        # function and pass down the arguments. The `forward` function is \n",
    "        # documented here: \n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "        # The results are returned in a results object, documented here:\n",
    "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
    "        # Specifically, we'll get the loss (because we provided labels) and the\n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "        # output values prior to applying an activation function like the \n",
    "        # softmax.\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d1bd19",
   "metadata": {
    "cellId": "6dt0baxplq3mnlv9hzrfta"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ead9588a",
   "metadata": {
    "cellId": "6e84fc9s6pa5vq3c2f6qk",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "6O_NbXFGMukX",
    "outputId": "a9e51eda-5eae-4800-87d5-8d016ff25bb2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.02</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0:02:39</td>\n",
       "      <td>0:00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.99</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0:02:29</td>\n",
       "      <td>0:00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.83</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0:02:30</td>\n",
       "      <td>0:00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.61</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0:02:30</td>\n",
       "      <td>0:00:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               3.02         3.00           0.05       0:02:39         0:00:31\n",
       "2               2.99         2.95           0.06       0:02:29         0:00:26\n",
       "3               2.83         2.68           0.13       0:02:30         0:00:26\n",
       "4               2.61         2.56           0.15       0:02:30         0:00:26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af634296",
   "metadata": {
    "cellId": "j1tio2s2ygv73g2pj1v5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "#!g1.1\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fec73e",
   "metadata": {
    "cellId": "irh6rvm2p3si97jg97bk"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "как будто можно еще обучить на несколько эпохах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3467acd0",
   "metadata": {
    "cellId": "q27xa8t99qh6dasowjykxh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aaefc624",
   "metadata": {
    "cellId": "4nt4jr4j9vqoqw3xucyn6j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 3,734\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2227: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "#!g1.1\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(X_test.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = X_test.text.values\n",
    "labels = y_test\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.  # max lenght IS SET TO 64!\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32    #BATCH SIZE!\n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b685d8",
   "metadata": {
    "cellId": "gvtcgad272m8twhn4mqq08"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1bc328f7",
   "metadata": {
    "cellId": "lns4fy8dlxapolc5qi5c6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hba10sXR7Xi6",
    "outputId": "e35f0a6e-72c5-4bd0-9c4b-dcec9ef5059d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 3,734 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions.\n",
    "      result = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     return_dict=True)\n",
    "\n",
    "  logits = result.logits\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae599e75",
   "metadata": {
    "cellId": "yvwseltupxxxyaozyq7e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCYZa1lQ8Jn8",
    "outputId": "b4650298-0e35-4ed8-be13-83f074a617ed"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09eaddfe",
   "metadata": {
    "cellId": "iyk2svqhbdjbxmtfm5insm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3734,), (3734,))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "flat_true_labels.shape, flat_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6778e963",
   "metadata": {
    "cellId": "f0w2bfm0qmd7nz4ojenkk"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1c52039",
   "metadata": {
    "cellId": "ebffhgh5gfjvqqt9pblono"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.121\n",
      "f1 0.08\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print('accuracy', np.round(accuracy_score(flat_true_labels, flat_predictions),3)) #качество на тесте\n",
    "print('f1', np.round(f1_score(flat_true_labels, flat_predictions, average='macro'),2)) #качество на тесте\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ab4c0",
   "metadata": {
    "cellId": "ftxg66dx5mk8idvc3gdvm8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "20956c6b-2e6e-436f-81fb-0625b6797a18",
  "notebookPath": "text_attacks/notebooks/2.0-20news-bert-fine-tuning.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
