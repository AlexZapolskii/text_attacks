{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de246039",
   "metadata": {
    "cellId": "0qxqnxov5zirs9zfoel9y"
   },
   "source": [
    "#!g1.1\n",
    "\n",
    "Загрузим датасет 20NewsGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a7bcb2d",
   "metadata": {
    "cellId": "2hdnmhzyrkycihd6i81rlo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "#загрузим обучающую выборку\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a93b74d",
   "metadata": {
    "cellId": "5waulwu83c7oc3mpf4bnu"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "X = newsgroups_train.data\n",
    "y = newsgroups_train.target\n",
    "\n",
    "X = pd.DataFrame(X)\n",
    "X.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9813b0",
   "metadata": {
    "cellId": "w0s3xyhmlzalq9dual17ss"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...\n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...\n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...\n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...\n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05aa5421",
   "metadata": {
    "cellId": "pbj9xlz29k5dzvjuqxpwj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4, 4, ..., 3, 1, 8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18f1dfa9",
   "metadata": {
    "cellId": "cocgvjx60p4ruwwhrqdxka"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11314, 1), (11314,))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d47e99d",
   "metadata": {
    "cellId": "kkv9j9bvh0m471s56kv9s4"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "всего 11к записей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad7dd822",
   "metadata": {
    "cellId": "m3pyc9dvynkcrd5nje4aaj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: jae2001@andy.bgsu.edu (Jason Ehas)\n",
      "Subject: Re: Giveaways\n",
      "Organization: Home of 1984 NCAA hockey champs\n",
      "Lines: 43\n",
      "\n",
      "In article <1qi44l$kqr@access.digex.net>, steveg@cadkey.com (Steve\n",
      "Gallichio) wrote:\n",
      "> \n",
      "> \n",
      "> John P. Curcio (jpc@philabs.philips.com) responded to my drivel:\n",
      "> \n",
      "> >steveg@cadkey.com (Steve Gallichio) writes:\n",
      "> > \n",
      "> >>I still am surprised that no one has tried giving away the goodies at the end\n",
      "> >>of the game. The two problems with that, of course, are that you would want\n",
      "> >>to make sure the first people in the building would be assured of getting\n",
      "> >>them (probably redeemable vouchers), and that the building managers want to\n",
      "> >>avoid at all costs delaying people as they leave the building, if, for\n",
      "> >>instance, the goodies are given to people as they exit.\n",
      "> >\n",
      "> >I went to the New Jersey Devils/Carvel Ice Cream Puck Night (tm) last year to\n",
      "> >see the beloved Bruins play.  The pucks were given out at the end of the game.\n",
      "> >I could just imagine what would have happened late in the third if the Bruins\n",
      "> >were winning....\n",
      "> \n",
      "> It figures, after I posted the first article, I found out that the Whalers are\n",
      "> going to be using coupons for the the giveaway on Friday Night. I believe that\n",
      "> is is the \"Some Big Corporation (Probably a Bank) Flying Disk Night.\" I think\n",
      "> that we could all see the potential for danger here...\n",
      "> \n",
      "> >|> All in all, I have seen a whole bunch of giveaways land on the ice, and it\n",
      "> >|> never ceases to amuse me. I'm just thankful for the players that no one has\n",
      "> >|> yet to sponsor 'Lead Pipe Night' at any arenas...\n",
      "> >\n",
      "> >That's probably because they couldn't find anyone to sponser it... Maybe USS\n",
      "> >could sponser the Pittsburgh Penguins/US Steel Steel Rod Night-- close enough?\n",
      "> \n",
      "> Naah, it'd probably bounce off of Jay Caufield.\n",
      "> \n",
      "> -SG\n",
      "\n",
      "I was at a Cincinnati Cyclones game a year ago when the local country\n",
      "station sponsored a kazoo giveaway.  After a particularly bad call by the\n",
      "underexperienced ECHL ref, it was Kazoostorm time down on the ice.  I\n",
      "thought this was a pathetic display by the fans, but they were rightfully\n",
      "unhappy.\n",
      "\n",
      "Jason\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "#пример текста \n",
    "\n",
    "print(X.text[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb2fb7",
   "metadata": {
    "cellId": "676e4wckz873zw7d0k8qk7"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64b05992",
   "metadata": {
    "cellId": "tjdpcgctiznzex56lqktc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rec.sport.hockey'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "#соответствующая категория:\n",
    "newsgroups_train.target_names[y[300]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ef9db5a",
   "metadata": {
    "cellId": "0cawyzkrt959lsmodvp5bl"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "863732c4",
   "metadata": {
    "cellId": "9ac8fr7owipuwknowuii5"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "#разобьем на трейн/тест \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  stratify = y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f37a6bc",
   "metadata": {
    "cellId": "xv2n7ufcra9ej0aslkkk2n"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0ded0c4",
   "metadata": {
    "cellId": "keu1ml56uicys13cxq5k2l",
    "id": "GuE5BqICAne2"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Get the lists of sentences and their labels.\n",
    "sentences = X_train.text.values\n",
    "labels = y_train\n",
    "\n",
    "#valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b2321",
   "metadata": {
    "cellId": "2jcjjw5428kcwg5ripdxr7"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e436871a",
   "metadata": {
    "cellId": "fl2bxfjepy9oxj8ep9yo9",
    "id": "GuE5BqICAne2"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Get the lists of sentences and their labels.\n",
    "sentences = train_df.text.values\n",
    "labels = train_df.label.values\n",
    "\n",
    "#valid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38748824",
   "metadata": {
    "cellId": "b3x2hw6jdnpr9sd619rzf"
   },
   "source": [
    "берем модель дистилированного Берта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "815e12bd",
   "metadata": {
    "cellId": "2pm5e95vz3nrb0phg6w7a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "82ddfcea0e4c4e5a86cf6eca8585be8d",
      "8a256ba4a19e4ec98fe3c3c99fba4daa",
      "8c76faadf2f4415393c6f0a805f0d72b",
      "e0bb735fda99434a90380e7fc664212d",
      "cdb78e75309f4bc09366533331e72431",
      "1058e0b5baa248faa60c1ad146d10bf7",
      "375cc635389c4ddb9bf2aa443df58bae",
      "472198d5b6a748b3a81f9364fd1fa711"
     ]
    },
    "id": "Z474sSC6oe7A",
    "outputId": "4e6d97b6-2d4c-42ca-c201-d2b4a88895b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126e6fb4db464937935582ea5ac636c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc815f1222a493696ddcaecaf581c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f8b002a4314e6bb76f6afe06f9ea87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5aba1dfc92429c9eb03a818f7637de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=483.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# попробуем облегченную модель \n",
    "# Load the BERT tokenizer. distilbert-base-uncased\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)  #стандартная модель - bert-base-uncased, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75296d",
   "metadata": {
    "cellId": "ovuu8csi7cf9iqtkmqjn7b"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "Оценим распределение длины текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be633f84",
   "metadata": {
    "cellId": "lq8kc1e4u0945765308h99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  52947\n",
      "CPU times: user 1min 3s, sys: 92 ms, total: 1min 3s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%%time\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80216ee7",
   "metadata": {
    "cellId": "un00qynlmxgf0nfs6e3smk"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    " мы видим - что макс длина последоательности 52к "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3c76cbf",
   "metadata": {
    "cellId": "hgxeki50haqf55v3te4wdn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     7580.000000\n",
       "mean       287.968734\n",
       "std        529.736149\n",
       "min         17.000000\n",
       "25%        108.000000\n",
       "50%        177.000000\n",
       "75%        294.000000\n",
       "max      11278.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "# тем не менее, важно понимать распределение кол-ва токенов в последовательности:\n",
    "\n",
    "# statistics of text length (in words)\n",
    "X_train['text'].apply(lambda s: len(s.split())).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c4b79ae",
   "metadata": {
    "cellId": "aac00glhe24r2lwk7dceta"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='text'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "# тем не менее, важно понимать распределение кол-ва токенов в последовательности:\n",
    "import seaborn as sns\n",
    "# statistics of text length (in words)\n",
    "sns.boxplot(X_train['text'].apply(lambda s: len(s.split())), showfliers=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4f798f3",
   "metadata": {
    "cellId": "bnn7mbekw5fotg939mejzg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769.0\n",
      "2004.42\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print(X_train['text'].apply(lambda s: len(s.split())).quantile(0.95))\n",
    "print(X_train['text'].apply(lambda s: len(s.split())).quantile(0.99))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "176895d7",
   "metadata": {
    "cellId": "05p3gc8eu7gio490c2t1unb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512.0\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print(X_train['text'].apply(lambda s: len(s.split())).quantile(0.90))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33512f07",
   "metadata": {
    "cellId": "8muy236wn044petkltfwip"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "` мы видим, что 90% данных укладываются в стандартную длину последовательности для Берта"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d41b4f6",
   "metadata": {
    "cellId": "5rjdfwipuead7vctxan0a"
   },
   "source": [
    "#!g1.1\n",
    "## Токенизируем "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a6783",
   "metadata": {
    "cellId": "i2b3l4sb9fg6rmeek1qvo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "выбрали длину последовательности - равной 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9fc62c58",
   "metadata": {
    "cellId": "kzvwp3us6uby9ot9rxc2aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kernel/lib/python3.7/site-packages/ml_kernel/kernel.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ml_kernel.logger import IOPubLogger, Logger2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  From: nlu@Xenon.Stanford.EDU (Nelson Lu)\n",
      "Subject: SHARKS REVIEW Part 3: Defensemen (21-45)\n",
      "Organization: Computer Science Department, Stanford University.\n",
      "Lines: 85\n",
      "\n",
      "#21\tPETER AHOLA\t\tSeason: 2nd\n",
      "Acquired:\t'92-93, trade with Pittsburgh for future considerations\n",
      "Grade:\t\tI (B)\n",
      "\n",
      "It is way too early to tell about Ahola, who was acquired probably because the\n",
      "Penguins figured that they would lose him in the expansion draft.  Ahola had\n",
      "only played 50 games this season (I think it's actually less; the San Jose\n",
      "Mercury News may be in err here), 20 of them with the Sharks.  In the games he\n",
      "has played, he appeared quite solid defensively, although he hasn't been\n",
      "spectacular, and his offense isn't anything to write home about (8 points);\n",
      "it's even possible that the trade may be for future considerations which turn\n",
      "out to be ... Peter Ahola.\n",
      "\n",
      "#24\tDOUG WILSON\t\tSeason: 16th\n",
      "Acquired:\t'91-92, trade with Chicago for RW Kerry Toporowski and\n",
      "\t\t2nd round pick in '92 entry draft\n",
      "Grade:\t\tI (B)\n",
      "\n",
      "I have often been accused of overly down on Wilson; I may have had too high\n",
      "expectations for him, but his legs, knees, et al., are giving out.\n",
      "Nevertheless, when he was playing, he exhibited a strong shooting and\n",
      "playmaking abilities, even if he has lost a step on defense, which,\n",
      "unfortunately, he demonstrated this year as well, as at times he was slow to\n",
      "catch the opponent forwards, and his offensive output was only good enough for\n",
      "2nd place on the team (20 points in 42 games).  But next year, which may be\n",
      "Wilson's last, if he can stay healthy, he can still be a contributor.\n",
      "\n",
      "#29\tDEAN KOLSTAD\t\tSeason:\t2nd\n",
      "Acquired:\t'91-92, from Minnesota in dispersal draft\n",
      "Grade:\t\tI (C-/D+)\n",
      "\n",
      "It's probably somewhat unfair for me to judge Kolstad on just a handful of\n",
      "games (forgetting exact number, but no more than 15), but at age 25 he's\n",
      "quickly running out of time if he wants to make it to the NHL.  In those games,\n",
      "he did not impress anyone; after generating 7 shots in the first period of\n",
      "the first game he played, he scored just 2 points in his tenure up here with\n",
      "the Sharks, and was even less impressive defensively, as he appeared awkward\n",
      "with his movement and was prone to giveaways.  He needs to make a leap in\n",
      "his level of performance to have any chance of making the team.\n",
      "\n",
      "#38\tPAT MACLEOD\t\tSeason: 2nd\n",
      "Acquired:\t'91-92, from Minnesota in dispersal draft\n",
      "Grade:\t\tI (?)\n",
      "\n",
      "MacLeod was on the roster a lot longer than Kolstad, but it appears to my\n",
      "memory that he played less than Kolstad, because the Sharks were reluctant to\n",
      "use him, but were even more reluctant to send him to the minors, figuring that\n",
      "he wouldn't clear waivers; in fact, he has played the past 4-5 weeks with\n",
      "Kansas City, but is still technically there on a rehabilitation assignment,\n",
      "a \"rehab assignment\" that will include him playing in the Turner Cup playoffs.\n",
      "Since he has played so little, I can't even give a tentative grade on him, but\n",
      "he demonstrated last year excellent offensive skills but terrible defensive\n",
      "skills.\n",
      "\n",
      "#41\tTOM PEDERSON\t\tSeason: 1st\n",
      "Acquired:\t'91-92, from Minnesota in dispersal draft\n",
      "Grade:\t\tI (B+)\n",
      "\n",
      "Called up in the middle of the season when the defensive corps was decimated\n",
      "by injuries, Pederson impressed many Sharks fan here on net, including yours\n",
      "truly.  He demonstrated very good offensive skills, scoring 20 points in\n",
      "43 games.  However, his size (5' 9\", 165 lbs.) is of concern, and soon after\n",
      "he began to shine offensive did teams begin to push him around physically,\n",
      "on both sides of the ice, although he had appeared fearless in his approach.\n",
      "But to be successful, he probably needs to bulk up to have a fighting chance\n",
      "on surviving against some of the bigger players in the league.\n",
      "\n",
      "#45\tCLAUDIO SCREMIN\t\tSeason: 1st\n",
      "Acquired:\t'91-92, from Minnesota in dispersal draft\n",
      "Grade:\t\tI (D+/D)\n",
      "\n",
      "He played all of ~5 games in the league this year, but was thoroughly\n",
      "umimpressive, just as he was at the end of last season; again, it may be a\n",
      "small sample, but just as in the case of Kolstad, Scremin, at age 25, is\n",
      "quickly running out of time.  He was not a contributor on either offense or\n",
      "defense in the games he played with the Sharks.  The only notable thing that\n",
      "will go down in Scremin's entry of league stats is probably the fact that he\n",
      "was once traded for now Capitals goaltender Don Beaupre.\n",
      "\n",
      "===============================================================================\n",
      "GO CALGARY FLAMES!  Al MacInnis for Norris!  Gary Roberts for Hart and Smythe!\n",
      "GO EDMONTON OILERS!  Go for playoffs next year!  Stay in Edmonton!\n",
      "===============================================================================\n",
      "Nelson Lu (claudius@leland.stanford.edu)\n",
      "rec.sport.hockey contact for the San Jose Sharks\n",
      "\n",
      "Token IDs: tensor([  101,  2013,  1024, 17953,  2226,  1030,  1060, 16515,  2078,  1012,\n",
      "         8422,  1012,  3968,  2226,  1006,  5912, 11320,  1007,  3395,  1024,\n",
      "        12004,  3319,  2112,  1017,  1024,  3639,  3549,  1006,  2538,  1011,\n",
      "         3429,  1007,  3029,  1024,  3274,  2671,  2533,  1010,  8422,  2118,\n",
      "         1012,  3210,  1024,  5594,  1001,  2538,  2848,  6289,  6030,  2161,\n",
      "         1024,  3416,  3734,  1024,  1005,  6227,  1011,  6109,  1010,  3119,\n",
      "         2007,  6278,  2005,  2925, 16852,  3694,  1024,  1045,  1006,  1038,\n",
      "         1007,  2009,  2003,  2126,  2205,  2220,  2000,  2425,  2055,  6289,\n",
      "         6030,  1010,  2040,  2001,  3734,  2763,  2138,  1996, 18134,  6618,\n",
      "         2008,  2027,  2052,  4558,  2032,  1999,  1996,  4935,  4433,  1012,\n",
      "         6289,  6030,  2018,  2069,  2209,  2753,  2399,  2023,  2161,  1006,\n",
      "         1045,  2228,  2009,  1005,  1055,  2941,  2625,  1025,  1996,  2624,\n",
      "         4560,  8714,  2739,  2089,  2022,  1999,  9413,  2099,  2182,  1007,\n",
      "         1010,  2322,  1997,  2068,  2007,  1996, 12004,  1012,  1999,  1996,\n",
      "         2399,  2002,  2038,  2209,  1010,  2002,  2596,  3243,  5024,  5600,\n",
      "         2135,  1010,  2348,  2002,  8440,  1005,  1056,  2042, 12656,  1010,\n",
      "         1998,  2010, 10048,  3475,  1005,  1056,  2505,  2000,  4339,  2188,\n",
      "         2055,  1006,  1022,  2685,  1007,  1025,  2009,  1005,  1055,  2130,\n",
      "         2825,  2008,  1996,  3119,  2089,  2022,  2005,  2925, 16852,  2029,\n",
      "         2735,  2041,  2000,  2022,  1012,  1012,  1012,  2848,  6289,  6030,\n",
      "         1012,  1001,  2484,  8788,  4267,  2161,  1024,  5767,  3734,  1024,\n",
      "         1005,  6205,  1011,  6227,  1010,  3119,  2007,  3190,  2005,  1054,\n",
      "         2860, 11260,  2327, 14604, 10344,  1998,  3416,  2461,  4060,  1999,\n",
      "         1005,  6227,  4443,  4433,  3694,  1024,  1045,  1006,  1038,  1007,\n",
      "         1045,  2031,  2411,  2042,  5496,  1997, 15241,  2091,  2006,  4267,\n",
      "         1025,  1045,  2089,  2031,  2018,  2205,  2152, 10908,  2005,  2032,\n",
      "         1010,  2021,  2010,  3456,  1010,  5042,  1010,  3802,  2632,  1012,\n",
      "         1010,  2024,  3228,  2041,  1012,  6600,  1010,  2043,  2002,  2001,\n",
      "         2652,  1010,  2002,  8176,  1037,  2844,  5008,  1998,  2377, 12614,\n",
      "         7590,  1010,  2130,  2065,  2002,  2038,  2439,  1037,  3357,  2006,\n",
      "         3639,  1010,  2029,  1010,  6854,  1010,  2002,  7645,  2023,  2095,\n",
      "         2004,  2092,  1010,  2004,  2012,  2335,  2002,  2001,  4030,  2000,\n",
      "         4608,  1996,  7116, 19390,  1010,  1998,  2010,  5805,  6434,  2001,\n",
      "         2069,  2204,  2438,  2005,  3416,  2173,  2006,  1996,  2136,  1006,\n",
      "         2322,  2685,  1999,  4413,  2399,  1007,  1012,  2021,  2279,  2095,\n",
      "         1010,  2029,  2089,  2022,  4267,  1005,  1055,  2197,  1010,  2065,\n",
      "         2002,  2064,  2994,  7965,  1010,  2002,  2064,  2145,  2022,  1037,\n",
      "        12130,  1012,  1001,  2756,  4670, 12849,  4877, 17713,  2161,  1024,\n",
      "         3416,  3734,  1024,  1005,  6205,  1011,  6227,  1010,  2013,  5135,\n",
      "         1999, 27602,  4433,  3694,  1024,  1045,  1006,  1039,  1011,  1013,\n",
      "         1040,  1009,  1007,  2009,  1005,  1055,  2763,  5399, 15571,  2005,\n",
      "         2033,  2000,  3648, 12849,  4877, 17713,  2006,  2074,  1037,  9210,\n",
      "         1997,  2399,  1006, 17693,  6635,  2193,  1010,  2021,  2053,  2062,\n",
      "         2084,  2321,  1007,  1010,  2021,  2012,  2287,  2423,  2002,  1005,\n",
      "         1055,  2855,  2770,  2041,  1997,  2051,  2065,  2002,  4122,  2000,\n",
      "         2191,  2009,  2000,  1996,  7097,  1012,  1999,  2216,  2399,  1010,\n",
      "         2002,  2106,  2025, 17894,  3087,  1025,  2044, 11717,  1021,  7171,\n",
      "         1999,  1996,  2034,  2558,  1997,  1996,  2034,  2208,  2002,  2209,\n",
      "         1010,  2002,  3195,  2074,  1016,  2685,  1999,  2010,  7470,  2039,\n",
      "         2182,  2007,  1996, 12004,  1010,  1998,  2001,  2130,  2625,  8052,\n",
      "         5600,  2135,  1010,  2004,  2002,  2596,  9596,  2007,  2010,  2929,\n",
      "         1998,   102])\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "#!g1.1\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 512,           # Pad & truncate all sentences.   Установили на основе анализа длины текстов\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f61c3f",
   "metadata": {
    "cellId": "36z03wsoqoy7m58vl4pu7e"
   },
   "source": [
    "#!g1.1\n",
    "\n",
    "## Training & Validation Split¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b73174c2",
   "metadata": {
    "cellId": "226c3inrsg27jflubimdgb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,822 training samples\n",
      "  758 validation samples\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "#!g1.1\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a55555ac",
   "metadata": {
    "cellId": "x6k3kv9qhieelymfpw45r",
    "id": "XGUqOCtgqGhP"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32  # посмотрим, как зайдет по памяти 64   ВАЖНО!  - нужно задавать гиперпараметры - как аргументы\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc6c5f",
   "metadata": {
    "cellId": "d4mez6xqjwc5gxjat9s0g4"
   },
   "source": [
    "#!g1.1\n",
    "Загрузим модель и укажем кол-во классов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef0e24d7",
   "metadata": {
    "cellId": "1zeh7dezfb6vmvk6f2vuic"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "num_classes = pd.Series(y).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e56ce68b",
   "metadata": {
    "cellId": "hgav6slj0sbgbyp0owa45e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5dd63e",
   "metadata": {
    "cellId": "zvfeuakaji8pkoh9bpdmf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "bf9dfa1ff3e642fbb74c5146d21044c2",
      "1c2b0ede959142fc89bf07a9c88df638",
      "1296a3d754b344a482a03e5af84e805e",
      "6f132d7bb83d41b6847df0d0ec0a1b92",
      "2755b9838bae408ca8cf667ad9d501fc",
      "f8874fec8a404ae89a38fd2ecbb357cf",
      "a7bdbedc75de4f77b45f1389c2ea0abc",
      "978c24b18b594eaf8ca47730a88eefb9",
      "fe254c3bcc08402eb506f0e98f5673a7",
      "cea84f9c3db641acb98314028b305514",
      "23ca9359e6c44232a1346e6f2ab7e48c",
      "d689bc8d488a4dc09c393b4fc9747bcb",
      "6c7dec7b1e804c2195f6e60fb3c1d18e",
      "0fe5b1d0540240a8a8426352c24b2887",
      "4b1e27aff6f04fec8268d951e46b1e63",
      "440da34c72344cb08e4a1ee5de7049ee"
     ]
    },
    "id": "gFsCTp_mporB",
    "outputId": "af690f33-6cd5-4678-bdaf-209f068f70f5"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "#НУЖНО УКАЗАТЬ КОЛ-ВО КЛАССОВ\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab. используем дистилированный берт\n",
    "    num_labels = num_classes, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16afc8be",
   "metadata": {
    "cellId": "sthzvl96v441ax4p3skao"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "#объявим вспомогателные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9c3b7e40",
   "metadata": {
    "cellId": "mss9oxaqailcioqmms7nha",
    "id": "9cQNvaZ9bnyy"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "#!g1.1\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4    # Кол-во эпох!\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "#!g1.1\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1cf380",
   "metadata": {
    "cellId": "txjqspcs7fminq3obovui"
   },
   "source": [
    "## Запускаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2510e45",
   "metadata": {
    "cellId": "cem7tki9y4fxbmmubgvi1d",
    "id": "gpt6tR83keZD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 864.00 MiB (GPU 0; 31.75 GiB total capacity; 29.76 GiB already allocated; 591.50 MiB free; 29.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-d0a0fceac712>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m                        \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                        \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                        return_dict=True)\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1547\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1549\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1550\u001b[0m         )\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1007\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m         )\n\u001b[1;32m   1011\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m                 )\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         )\n\u001b[1;32m    479\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;31m# Mask heads if we want to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 864.00 MiB (GPU 0; 31.75 GiB total capacity; 29.76 GiB already allocated; 591.50 MiB free; 29.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "#!g1.1\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
    "        # function and pass down the arguments. The `forward` function is \n",
    "        # documented here: \n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "        # The results are returned in a results object, documented here:\n",
    "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
    "        # Specifically, we'll get the loss (because we provided labels) and the\n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "        # output values prior to applying an activation function like the \n",
    "        # softmax.\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6237f67",
   "metadata": {
    "cellId": "6dt0baxplq3mnlv9hzrfta"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "20956c6b-2e6e-436f-81fb-0625b6797a18",
  "notebookPath": "text_attacks/notebooks/2.0-20news-bert-fine-tuning.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
