{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c6925",
   "metadata": {
    "cellId": "mxb7wq7g39yupmds832p"
   },
   "outputs": [],
   "source": [
    "# Работа состоит из несольких частей\n",
    "    - код\n",
    "    - гипотезы\n",
    "    - текст\n",
    "    - обзор литературы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf85b5fc",
   "metadata": {
    "cellId": "cl8nhnpbrfzi92kgpqgfr"
   },
   "source": [
    "Мы хотим оценить влияние различных адверсариальных атак на современные NLP модели\n",
    "\n",
    "В рамках данного исследования мы остановимся на задаче классификации\n",
    "\n",
    "Следовательно: нам нужны\n",
    "    - Датасеты для классификации\n",
    "        - 2 на Английском (common domain, specific domain)\n",
    "        - 2 на Русском (common domain, specific domain)\n",
    "        \n",
    "    - Модели для классификации\n",
    "        - Мы можем использовать TF-IDF на log-reg\n",
    "        - Bert и его разновидности\n",
    "            - Английский Берт (Bert-base-uncased, distilled-bert-uncades)\n",
    "            - Мультиязычный Берт ?\n",
    "            - Русский Берт (DeepPavlov, дистилированная модель от Давида Деле)\n",
    "            \n",
    "    - Атаки:\n",
    "        - BAE\n",
    "        - TextFooler\n",
    "        - Другие атаки из модуля TextAtack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55ec05d",
   "metadata": {
    "cellId": "gu2rw9225rbiz3z3xadz19"
   },
   "source": [
    "Постановка эксперимента:\n",
    "    - Пока что не трогаем лог-рег\n",
    "    - Исследуем Берт\n",
    "    \n",
    "Задача1:\n",
    "    - fine-tuning Берта под задачу классификации\n",
    "    - Оценка качества на валидации\n",
    "    - Подготовка адверсариальных примеров на основе валидационного датасета\n",
    "    - Оценка качества на адверсариальных примерах"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea48d9e",
   "metadata": {
    "cellId": "saakxkkmsbgh8vdvhyfv9"
   },
   "source": [
    "Этапы работы:\n",
    "    1) Загрузка и препроцессинг данных\n",
    "    2) Fine-tuning соответствующей модели\n",
    "    3) Валидация\n",
    "    4) Генерация адверсариальных примеров\n",
    "    5) Оценка качества\n",
    "        - Автоматическая валидация:\n",
    "            - accuracy\n",
    "            - semantic score\n",
    "        - Human evaluation\n",
    "            - Классификация примеров\n",
    "            - Оценка \"реалистичности и грамотности сгенерированных примеров\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b422fef",
   "metadata": {
    "cellId": "z4hc8m4yopggtq9bgw80cn"
   },
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e4319",
   "metadata": {
    "cellId": "x2q5a1mdq58xokxb06k2o"
   },
   "outputs": [],
   "source": [
    "построим пайплайн на основе ноутбука https://www.kaggle.com/kashnitsky/distillbert-catalyst-amazon-product-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c85f4",
   "metadata": {
    "cellId": "0fzx6zll9zpmb3yh2ntq43n"
   },
   "source": [
    "%pip install -U catalyst transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7605a8a9",
   "metadata": {
    "cellId": "qf9fyonkdequd7zdy63eyg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U catalyst transformers > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0829976",
   "metadata": {
    "cellId": "3tfarpkl4cvlsf0f6t5pe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/catalyst-team/catalyst@master\n",
      "  Cloning https://github.com/catalyst-team/catalyst (to revision master) to /tmp/pip-req-build-ohzvqj2r\n",
      "  Running command git clone -q https://github.com/catalyst-team/catalyst /tmp/pip-req-build-ohzvqj2r\n",
      "    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.3.0 in /home/jupyter/.local/lib/python3.7/site-packages (from catalyst==21.12rc0) (1.10.0)\n",
      "Requirement already satisfied: hydra-slayer>=0.1.1 in /home/jupyter/.local/lib/python3.7/site-packages (from catalyst==21.12rc0) (0.4.0)\n",
      "Requirement already satisfied: tensorboardX<2.3.0>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from catalyst==21.12rc0) (2.1)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from catalyst==21.12rc0) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.18 in /kernel/lib/python3.7/site-packages (from catalyst==21.12rc0) (1.19.4)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in /home/jupyter/.local/lib/python3.7/site-packages (from catalyst==21.12rc0) (4.49.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX<2.3.0>=2.1.0->catalyst==21.12rc0) (3.17.3)\n",
      "Requirement already satisfied: six in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboardX<2.3.0>=2.1.0->catalyst==21.12rc0) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions in /home/jupyter/.local/lib/python3.7/site-packages (from torch>=1.3.0->catalyst==21.12rc0) (3.7.4.3)\n",
      "Building wheels for collected packages: catalyst\n",
      "  Building wheel for catalyst (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for catalyst: filename=catalyst-21.12rc0-py3-none-any.whl size=536895 sha256=2d6a1dab6672161c1d8b48ab51250509504aad3a7ee4ca415cc065eb5b05e0b7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ld8v2rz7/wheels/91/6b/6a/134b3760024ee0155acd92bb4627ac0642505795c7cf20f204\n",
      "Successfully built catalyst\n",
      "Installing collected packages: catalyst\n",
      "  Attempting uninstall: catalyst\n",
      "    Found existing installation: catalyst 21.11\n",
      "    Uninstalling catalyst-21.11:\n",
      "      Successfully uninstalled catalyst-21.11\n",
      "\u001b[33m  WARNING: The scripts catalyst-contrib and catalyst-dl are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed catalyst-21.12rc0\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/catalyst-team/catalyst@master --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b751d3c8",
   "metadata": {
    "cellId": "tlbnzbgdhem5c8r3uyjy68"
   },
   "outputs": [],
   "source": [
    "# Python \n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "from typing import Mapping, List\n",
    "from pprint import pprint\n",
    "\n",
    "# Numpy and Pandas \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformers \n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "# Catalyst\n",
    "from catalyst.dl import SupervisedRunner\n",
    "#from catalyst.dl.callbacks import AccuracyCallback, F1ScoreCallback, OptimizerCallback\n",
    "#from catalyst.dl.callbacks import CheckpointCallback, InferCallback\n",
    "from catalyst.utils import set_global_seed, prepare_cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "822b82e9",
   "metadata": {
    "cellId": "te6zgj640xibz5syst30or"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased' # pretrained model from Transformers\n",
    "LOG_DIR = \"./logdir_amazon_reviews\"    # for training logs and tensorboard visualizations\n",
    "NUM_EPOCHS = 3                         # smth around 2-6 epochs is typically fine when finetuning transformers\n",
    "BATCH_SIZE = 72                        # depends on your available GPU memory (in combination with max seq length)\n",
    "MAX_SEQ_LENGTH = 256                   # depends on your available GPU memory (in combination with batch size)\n",
    "LEARN_RATE = 5e-5                      # learning rate is typically ~1e-5 for transformers\n",
    "ACCUM_STEPS = 4                        # one optimization step for that many backward passes\n",
    "SEED = 17                              # random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a9a9a",
   "metadata": {
    "cellId": "fd7cyh32q6km5krexy3p"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fc27a4",
   "metadata": {
    "cellId": "ihbd8f1g5dp6c9wd0dv2el"
   },
   "outputs": [],
   "source": [
    "\n",
    "Amazon product reviews - competition. Given text of a review, we need to classify it into one of 6 categories: dogs, cats, fish aquatic pets, birds, and two others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859055f",
   "metadata": {
    "cellId": "6uruu1o38q5q3xjhcr808p"
   },
   "outputs": [],
   "source": [
    "#!unzip data/amazon-pet-product-reviews-classification.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9380e78f",
   "metadata": {
    "cellId": "74v18vgr8bsxh2ul5jhcc"
   },
   "outputs": [],
   "source": [
    "# to reproduce, download the data and customize this path\n",
    "PATH_TO_DATA = 'data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82b11397",
   "metadata": {
    "cellId": "1prtmddjoia6f5cyapdll6"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(PATH_TO_DATA + 'train.csv', index_col='id').fillna('')\n",
    "valid_df = pd.read_csv(PATH_TO_DATA + 'valid.csv', index_col='id').fillna('')\n",
    "test_df = pd.read_csv(PATH_TO_DATA + 'test.csv', index_col='id').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e134092f",
   "metadata": {
    "cellId": "11julyzvcloc5098xdvmvf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sam has an everlast treat each nite before bed...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The product is as it says. I keep an eye on it...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My Kitty thinks these are treats! He loves the...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is the third or fourth time that we've or...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Put this on both my dogs. And they are scratch...</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text label\n",
       "id                                                         \n",
       "0   Sam has an everlast treat each nite before bed...  dogs\n",
       "1   The product is as it says. I keep an eye on it...  dogs\n",
       "2   My Kitty thinks these are treats! He loves the...  dogs\n",
       "3   This is the third or fourth time that we've or...  dogs\n",
       "4   Put this on both my dogs. And they are scratch...  dogs"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31723d03",
   "metadata": {
    "cellId": "5nu4l4wtdromryfwhx0wkp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dogs                    0.537872\n",
       "cats                    0.355284\n",
       "fish aquatic pets       0.069001\n",
       "birds                   0.020324\n",
       "bunny rabbit central    0.010950\n",
       "small animals           0.006570\n",
       "Name: label, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# target distribution\n",
    "train_df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "695ffe1c",
   "metadata": {
    "cellId": "mj6ki5shtwpw4fqryvrpp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    52057.000000\n",
       "mean        84.420443\n",
       "std         80.027988\n",
       "min          1.000000\n",
       "25%         35.000000\n",
       "50%         61.000000\n",
       "75%        106.000000\n",
       "max       2360.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# statistics of text length (in words)\n",
    "train_df['text'].apply(lambda s: len(s.split())).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd1cd3c",
   "metadata": {
    "cellId": "l01o9p40c2go8pcbzuew1"
   },
   "source": [
    "## Torch Dataset\n",
    "This is left for user to be defined. Catalyst will take care of the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c121ff31",
   "metadata": {
    "cellId": "q9jkyhmmz7e7scmvlpq3"
   },
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wrapper around Torch Dataset to perform text classification\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 texts: List[str],\n",
    "                 labels: List[str] = None,\n",
    "                 label_dict: Mapping[str, int] = None,\n",
    "                 max_seq_length: int = 512,\n",
    "                 model_name: str = 'distilbert-base-uncased'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (List[str]): a list with texts to classify or to train the\n",
    "                classifier on\n",
    "            labels List[str]: a list with classification labels (optional)\n",
    "            label_dict (dict): a dictionary mapping class names to class ids,\n",
    "                to be passed to the validation data (optional)\n",
    "            max_seq_length (int): maximal sequence length in tokens,\n",
    "                texts will be stripped to this length\n",
    "            model_name (str): transformer model name, needed to perform\n",
    "                appropriate tokenization\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.label_dict = label_dict\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        if self.label_dict is None and labels is not None:\n",
    "            # {'class1': 0, 'class2': 1, 'class3': 2, ...}\n",
    "            # using this instead of `sklearn.preprocessing.LabelEncoder`\n",
    "            # no easily handle unknown target values\n",
    "            self.label_dict = dict(zip(sorted(set(labels)),\n",
    "                                       range(len(set(labels)))))\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # suppresses tokenizer warnings\n",
    "        logging.getLogger(\n",
    "            \"transformers.tokenization_utils\").setLevel(logging.FATAL)\n",
    "\n",
    "        # special tokens for transformers\n",
    "        # in the simplest case a [CLS] token is added in the beginning\n",
    "        # and [SEP] token is added in the end of a piece of text\n",
    "        # [CLS] <indexes text tokens> [SEP] .. <[PAD]>\n",
    "        self.sep_vid = self.tokenizer.vocab[\"[SEP]\"]\n",
    "        self.cls_vid = self.tokenizer.vocab[\"[CLS]\"]\n",
    "        self.pad_vid = self.tokenizer.vocab[\"[PAD]\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index) -> Mapping[str, torch.Tensor]:\n",
    "        \"\"\"Gets element of the dataset\n",
    "\n",
    "        Args:\n",
    "            index (int): index of the element in the dataset\n",
    "        Returns:\n",
    "            Single element by index\n",
    "        \"\"\"\n",
    "\n",
    "        # encoding the text\n",
    "        x = self.texts[index]\n",
    "        x_encoded = self.tokenizer.encode(\n",
    "            x,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_seq_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).squeeze(0)\n",
    "\n",
    "        # padding short texts\n",
    "        true_seq_length = x_encoded.size(0)\n",
    "        pad_size = self.max_seq_length - true_seq_length\n",
    "        pad_ids = torch.Tensor([self.pad_vid] * pad_size).long()\n",
    "        x_tensor = torch.cat((x_encoded, pad_ids))\n",
    "\n",
    "        # dealing with attention masks - there's a 1 for each input token and\n",
    "        # if the sequence is shorter that `max_seq_length` then the rest is\n",
    "        # padded with zeroes. Attention mask will be passed to the model in\n",
    "        # order to compute attention scores only with input data\n",
    "        # ignoring padding\n",
    "        mask = torch.ones_like(x_encoded, dtype=torch.int8)\n",
    "        mask_pad = torch.zeros_like(pad_ids, dtype=torch.int8)\n",
    "        mask = torch.cat((mask, mask_pad))\n",
    "\n",
    "        output_dict = {\n",
    "            \"features\": x_tensor,\n",
    "            'attention_mask': mask\n",
    "        }\n",
    "\n",
    "        # encoding target\n",
    "        if self.labels is not None:\n",
    "            y = self.labels[index]\n",
    "            y_encoded = torch.Tensor(\n",
    "                [self.label_dict.get(y, -1)]\n",
    "            ).long().squeeze(0)\n",
    "            output_dict[\"targets\"] = y_encoded\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7180ce",
   "metadata": {
    "cellId": "qfdor4l7w9q8m11y9h08f5"
   },
   "source": [
    "Create Torch Datasets with train, validation, and test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "815440ea",
   "metadata": {
    "cellId": "d4951pjwhvozr936kan3h"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4237dbb0dcb74093a94465806f8c8187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9975a69cb1d4992a5ec203f018c6680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=483.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc4e2e56c384ca5805a38d44a767f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5879d084818c4251bcbdff14b2b5aa62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextClassificationDataset(\n",
    "    texts=train_df['text'].values.tolist(),\n",
    "    labels=train_df['label'].values.tolist(),\n",
    "    label_dict=None,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "valid_dataset = TextClassificationDataset(\n",
    "    texts=valid_df['text'].values.tolist(),\n",
    "    labels=valid_df['label'].values.tolist(),\n",
    "    label_dict=train_dataset.label_dict,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "test_dataset = TextClassificationDataset(\n",
    "    texts=test_df['text'].values.tolist(),\n",
    "    labels=None,\n",
    "    label_dict=None,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    model_name=MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d229c13",
   "metadata": {
    "cellId": "sowykc4ff5m3g5cu8jtzyj"
   },
   "source": [
    "We infer the number of classes from the training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56935221",
   "metadata": {
    "cellId": "x8ny44bmhxr4zoj1su42ul"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(train_dataset.label_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0fb4cc4",
   "metadata": {
    "cellId": "r5arjk3yfjg54wamvgto6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     The product is as it says. I keep an eye on it...\n",
       "label                                                 dogs\n",
       "Name: 1, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.loc[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24b2963d",
   "metadata": {
    "cellId": "yqifo4t9cfh2wpbq2ekn66"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int8),\n",
      " 'features': tensor([  101,  1996,  4031,  2003,  2004,  2009,  2758,  1012,  1045,  2562,\n",
      "         2019,  3239,  2006,  2009,  2000,  2191,  2469,  2009,  2987,  2102,\n",
      "         2175,  2125,  2302,  3114,  2295,  1010,  2009,  2052,  2022,  3243,\n",
      "         8796,  2005,  1996, 26781,  2015,  1010,  2021,  2009,  2003,  2823,\n",
      "         2734,  1999, 11681,  1012,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0]),\n",
      " 'targets': tensor(3)}\n"
     ]
    }
   ],
   "source": [
    "pprint(train_dataset[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef36a8",
   "metadata": {
    "cellId": "fy07kzfs1b5cwo4qw017o"
   },
   "source": [
    "Finally, we define standard PyTorch loaders. This dictionary will be fed to Catalyst.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1574e13",
   "metadata": {
    "cellId": "26mq9xdwzw7na1e34bckhe"
   },
   "outputs": [],
   "source": [
    "train_val_loaders = {\n",
    "    \"train\": DataLoader(dataset=train_dataset,\n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=True),\n",
    "    \"valid\": DataLoader(dataset=valid_dataset,\n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=False)    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3156933b",
   "metadata": {
    "cellId": "fsghxlzusgr0g88z91twlk"
   },
   "source": [
    "## The model¶\n",
    "It's going to be a slightly simplified version of DistilBertForSequenceClassification by HuggingFace.<br> We need only predicted probabilities as output, nothing more - we don't need neither loss to be output nor hidden states or attentions (as in the original implementation).\n",
    "\n",
    "A good overview of DistilBERT is done in this great post by Jay Alammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "22349b0b",
   "metadata": {
    "cellId": "b3tfsy304to68o31c2cn26"
   },
   "outputs": [],
   "source": [
    "class DistilBertForSequenceClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified version of the same class by HuggingFace.\n",
    "    See transformers/modeling_distilbert.py in the transformers repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model_name: str, num_classes: int = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pretrained_model_name (str): HuggingFace model name.\n",
    "                See transformers/modeling_auto.py\n",
    "            num_classes (int): the number of class labels\n",
    "                in the classification task\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            pretrained_model_name, num_labels=num_classes)\n",
    "\n",
    "        self.distilbert = AutoModel.from_pretrained(pretrained_model_name,\n",
    "                                                    config=config)\n",
    "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
    "        self.classifier = nn.Linear(config.dim, num_classes)\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "\n",
    "    def forward(self, features, attention_mask=None, head_mask=None):\n",
    "        \"\"\"Compute class probabilities for the input sequence.\n",
    "\n",
    "        Args:\n",
    "            features (torch.Tensor): ids of each token,\n",
    "                size ([bs, seq_length]\n",
    "            attention_mask (torch.Tensor): binary tensor, used to select\n",
    "                tokens which are used to compute attention scores\n",
    "                in the self-attention heads, size [bs, seq_length]\n",
    "            head_mask (torch.Tensor): 1.0 in head_mask indicates that\n",
    "                we keep the head, size: [num_heads]\n",
    "                or [num_hidden_layers x num_heads]\n",
    "        Returns:\n",
    "            PyTorch Tensor with predicted class probabilities\n",
    "        \"\"\"\n",
    "        assert attention_mask is not None, \"attention mask is none\"\n",
    "        distilbert_output = self.distilbert(input_ids=features,\n",
    "                                            attention_mask=attention_mask,\n",
    "                                            head_mask=head_mask)\n",
    "        # we only need the hidden state here and don't need\n",
    "        # transformer output, so index 0\n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        # we take embeddings from the [CLS] token, so again index 0\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)  # (bs, dim)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5238b19",
   "metadata": {
    "cellId": "kn5snsjlpigevto9bopm3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a3947e7905349bbafe3251778cddd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification(pretrained_model_name=MODEL_NAME,\n",
    "                                            num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808ad880",
   "metadata": {
    "cellId": "uagcx6gelxl0n3y088hma5h"
   },
   "source": [
    "Model training\n",
    "First we specify optimizer and scheduler (pure PyTorch). Then Catalyst stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8731accf",
   "metadata": {
    "cellId": "slyajk64ikb8vg2crfkwoh"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARN_RATE)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "#заменим шидулер,  чтобы не ругался:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54fbcea",
   "metadata": {
    "cellId": "69tzcd7jbakpb4hogcq94"
   },
   "source": [
    "To run Deep Learning experiments, Catalyst resorts to the Runner abstraction, in particular, to SupervisedRunner.\n",
    "\n",
    "SupervisedRunner implements the following methods:\n",
    "\n",
    "train - starts the training process of the model\n",
    "predict_loader - makes a prediction on the whole loader with the specified model\n",
    "infer - makes the inference on the model\n",
    "To train the model within this interface you pass the following to the train method:\n",
    "\n",
    "model (torch.nn.Module) – PyTorch model to train\n",
    "criterion (nn.Module) – PyTorch criterion function for training\n",
    "optimizer (optim.Optimizer) – PyTorch optimizer for training\n",
    "loaders (dict) – dictionary containing one or several torch.utils.data.DataLoader for training and validation\n",
    "logdir (str) – path to output directory. There Catalyst will write logs, will dump the best model and the actual code to train the model\n",
    "callbacks – list of Catalyst callbacks\n",
    "scheduler (optim.lr_scheduler._LRScheduler) – PyTorch scheduler for training\n",
    "...\n",
    "In our case we'll pass the created DistilBertForSequenceClassification model, cross-entropy criterion, Adam optimizer, scheduler and data loaders that we created earlier. Also, we'll be tracking accuracy and thus will need AccuracyCallback. To perform batch accumulation, we'll be using OptimizationCallback.\n",
    "\n",
    "There are many more useful callbacks implemented, also check out Catalyst examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a743c3a",
   "metadata": {
    "cellId": "qjkd44yu6xica6mnfgg4yv"
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"    # can be changed in case of multiple GPUs onboard\n",
    "set_global_seed(SEED)                       # reproducibility\n",
    "prepare_cudnn(deterministic=True)           # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b276101",
   "metadata": {
    "cellId": "s65fyj316udpdywqrxz04"
   },
   "outputs": [],
   "source": [
    "!!На этом замечательно моменте все сломалось - из за того, что не смогли заимпортить\n",
    "\n",
    "from catalyst.dl.callbacks import AccuracyCallback, F1ScoreCallback, OptimizerCallback\n",
    "from catalyst.dl.callbacks import CheckpointCallback, InferCallback\n",
    "\n",
    "результат - ModuleNotFoundError: No module named 'catalyst.dl.callbacks'\n",
    "видимо, код изменился настолько, что с этой частью нужно разбираться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c59aadc",
   "metadata": {
    "cellId": "mzgwzqt70fy5138fn98fg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21.11'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import catalyst\n",
    "from catalyst import dl, metrics, utils\n",
    "catalyst.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c06fac",
   "metadata": {
    "cellId": "96fzzd90o5kwi44zzo4dto"
   },
   "outputs": [],
   "source": [
    "Запустили процесс на цпу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc238f3",
   "metadata": {
    "cellId": "v2se33w75obx8mmftxkj4"
   },
   "outputs": [],
   "source": [
    "ВОПРОС: и все таки, куда пропали callbacks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599af42e",
   "metadata": {
    "cellId": "z6rvwltvh77wlt48zh1ezn",
    "execution_id": "9f2a0b7e-5d17-4fa4-be5b-4e98f2fe8477"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "%%time\n",
    "# here we specify that we pass masks to the runner. So model's forward method will be called with\n",
    "# these arguments passed to it. \n",
    "runner = SupervisedRunner(\n",
    "    input_key=(\n",
    "        \"features\",\n",
    "        \"attention_mask\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# model training\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    loaders=train_val_loaders,\n",
    "   \n",
    "    logdir=LOG_DIR,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8228904",
   "metadata": {
    "cellId": "onljm74j4acfg0gmc80c"
   },
   "outputs": [],
   "source": [
    "посмотрим, сможет ли цпу за разумное время что то посчтитать?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d503f608",
   "metadata": {
    "cellId": "8pxx36oj40ni205j7cgtqf",
    "execution_id": "ba7a0437-daf2-45c0-b31c-c1c4ec33180c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06acd88141b41b4a50dba2e817ce122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='1/3 * Epoch (train)', max=724.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# here we specify that we pass masks to the runner. So model's forward method will be called with\n",
    "# these arguments passed to it. \n",
    "runner = SupervisedRunner(\n",
    "    input_key=(\n",
    "        \"features\",\n",
    "        \"attention_mask\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# model training\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    loaders=train_val_loaders,\n",
    "   \n",
    "    logdir=LOG_DIR,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b94a49",
   "metadata": {
    "cellId": "zi4hxyk3jper1g44p2s1h"
   },
   "source": [
    "вывод - на цпу ждать можно бесконечно!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1824a",
   "metadata": {
    "cellId": "poampgqqfvembnym2k88m"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "fcbde092-ab1e-471d-a905-bfb3fcb010a3",
  "notebookPath": "text_attacks/Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
